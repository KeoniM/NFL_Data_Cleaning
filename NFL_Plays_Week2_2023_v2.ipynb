{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BNhnIi0tediv",
        "2SemOF9G_cDS",
        "rkViqYZR_wtM",
        "pxVMKoLycv-O",
        "mXMEOEkUrQ9h",
        "rxluYvbU0t_t",
        "7CF2Mdx00_6w",
        "axDkGMWkNFut",
        "c4Q_lOfDWS5s",
        "dT8f80dqL8NV",
        "Worc0fONOglk",
        "84vArEhwLL8l",
        "RWtJnRCNXGah",
        "YY6Bv7t8LiaW",
        "f8THfFdPuayo",
        "-1ZMhEAyugit",
        "fdOg2g3qA2DH",
        "wa4M-wQNRy5r",
        "umFY2-YmILcC",
        "44hbpB7VviFL",
        "eX9zBvEMJFgT",
        "ZNlQga6VuzkJ",
        "rFRPLX5oPFE3",
        "NFZdKrzb52i-",
        "vwennCDeJLD8",
        "TdvXP3PDgVgt",
        "PWzncjhWJS-1",
        "Is4VUno1umTY",
        "VbIhpLjpwFkj",
        "piCO0D8ADw2p",
        "fbf9MCGcSuZ6",
        "02Km3sZUG9wI"
      ],
      "authorship_tag": "ABX9TyNdMKYIsVfmbrbKeBhtSjnm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KeoniM/NFL_Data_Cleaning/blob/main/NFL_Plays_Week2_2023_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PURPOSE:**\n",
        "- Accurately clean a week's worth of play data\n",
        "  - Season 2023 -> Week 2\n",
        "\n",
        "**NOTE:**\n",
        "- What makes version 2 different than version 1 is the data being used. Although the core of the data is identical to the original, NFL.com has updated their formatting of how they display their data which has been scraped and used here. So minor adjustments will have to be made in creating the new version but I also see a beautiful opportunity to clean the older version here. Make the code more readible, organized and efficient."
      ],
      "metadata": {
        "id": "NqOKml4zcVGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MOUNTING AND IMPORTS"
      ],
      "metadata": {
        "id": "BNhnIi0tediv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dnZLj2LAcNJd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "3c87e026-1245-4e73-d580-5b075b17e86e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3709497787.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mount your Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "# Mount your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to access personal google cloud services\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "metadata": {
        "id": "jJ685zOUekFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "# Data manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# Regular expressions\n",
        "import re\n",
        "\n",
        "# # Natural Language Toolkit (Used to find complete sentences)\n",
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('punkt_tab')\n",
        "# from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Database access\n",
        "from google.cloud import bigquery"
      ],
      "metadata": {
        "id": "mBKQGr7Ueozv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOADING DATA (BigQuery)"
      ],
      "metadata": {
        "id": "2SemOF9G_cDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Client connect to bigquery project\n",
        "client = bigquery.Client('nfl-data-430702')"
      ],
      "metadata": {
        "id": "aeXXx6B__myC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Season 2023 Week 2"
      ],
      "metadata": {
        "id": "rkViqYZR_wtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grabbing all plays from 2023 Week 2 NFL Sesason\n",
        "nfl_plays_week2_2023_query = \"\"\"\n",
        "                             SELECT *\n",
        "                             FROM `nfl-data-430702.NFL_Scores_v2.NFL-Plays-Week2_2023`\n",
        "                             \"\"\"\n",
        "\n",
        "# Running psuedo query, and returns the amount of bytes it will take to run query\n",
        "dry_run_config = bigquery.QueryJobConfig(dry_run=True)\n",
        "dry_run_query = client.query(nfl_plays_week2_2023_query, job_config=dry_run_config)\n",
        "print(\"This query will process {} gigabytes.\".format(dry_run_query.total_bytes_processed/10**9))\n",
        "\n",
        "# Running query (Being mindful of the amount of data being grabbed)\n",
        "# Will grab a maximum of a Gigabyte\n",
        "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**9)\n",
        "safe_config_query = client.query(nfl_plays_week2_2023_query, job_config=safe_config)"
      ],
      "metadata": {
        "id": "qDf8HGEU_zRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Putting data attained from query into a dataframe\n",
        "week2_2023_plays = safe_config_query.to_dataframe()"
      ],
      "metadata": {
        "id": "pFM5y64KBRja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "week2_2023_plays.head()"
      ],
      "metadata": {
        "id": "p7TChgOtCgYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CATEGORIZE PLAYS\n",
        "- The goal here is to parse out the different values for 'PlayOutcome'\n",
        "  - Here is where I will separate different types of plays\n",
        "    - ( pass / run / kickoff / etc..)"
      ],
      "metadata": {
        "id": "pxVMKoLycv-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All play outcomes from the game\n",
        "# - From here we can categorize and clean plays accordingly\n",
        "week2_2023_plays['PlayOutcome'].unique()"
      ],
      "metadata": {
        "id": "lmSflZLndCd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTES:\n",
        "# - Currently, I am eyeing all unique play outcomes to categorizing them.\n",
        "#   - This type of approach is not flexable because a play outcome can\n",
        "#     arise that has not been seen yet.\n",
        "#     - There may be more play outcomes in the future when working on a full season,\n",
        "#       let alone all seasons and future games\n",
        "\n",
        "# Play Types with complete cleaning methods (As far as this sample size goes)\n",
        "\n",
        "# ~ OFFENSE ~\n",
        "df_2023_pass_week2 = week2_2023_plays[week2_2023_plays['PlayOutcome'].str.contains('Pass')]\n",
        "df_2023_run_week2 = week2_2023_plays[week2_2023_plays['PlayOutcome'].str.contains('Run')]\n",
        "# ~ DEFENSE ~\n",
        "df_2023_interception_week2 = week2_2023_plays[week2_2023_plays['PlayOutcome'].str.contains('Interception')]\n",
        "df_2023_sack_week2 = week2_2023_plays[week2_2023_plays['PlayOutcome'].str.contains('Sack')]\n",
        "# ~ SPECIAL TEAMS ~\n",
        "df_2023_punt_week2 = week2_2023_plays[week2_2023_plays['PlayOutcome'].str.contains('Punt')]\n",
        "df_2023_kickoff_week2 = week2_2023_plays[week2_2023_plays['PlayOutcome'].str.contains('Kickoff')]\n",
        "# ~ SCORING ~\n",
        "df_2023_touchdown_week2 = week2_2023_plays[week2_2023_plays['PlayOutcome'].str.contains('Touchdown')]\n",
        "df_2023_extrapoint_week2 = week2_2023_plays[week2_2023_plays['PlayOutcome'].str.contains('Extra Point')]\n",
        "df_2023_fieldgoal_week2 = week2_2023_plays[week2_2023_plays['PlayOutcome'].str.contains('Field Goal')]\n",
        "# df_2023_2pt_conversion_week2 = week2_2023_plays[week2_2023_plays['PlayOutcome'].str.contains('2PT Conversion')]\n",
        "df_2023_2pt_conversion_week2 = week2_2023_plays[week2_2023_plays['PlayOutcome'].str.contains('Conversion')]\n",
        "# ~ OTHER ~\n",
        "df_2023_fumble_week2 = week2_2023_plays[week2_2023_plays['PlayOutcome'].str.contains('Fumble')]\n",
        "df_2023_penalty_week2 = week2_2023_plays[week2_2023_plays['PlayOutcome'].str.contains('Penalty')]\n",
        "df_2023_turnover_on_downs_week2 = week2_2023_plays[week2_2023_plays['PlayOutcome'].str.contains('Turnover on Downs')]\n",
        "df_2023_timeout_week2 = week2_2023_plays[week2_2023_plays['PlayOutcome'].str.contains('Timeout')]"
      ],
      "metadata": {
        "id": "HCnh3C4TdRjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SANITY CHECK (All Plays Accounted for)\n",
        "  - Once all plays have been categorized, will compare the sum of all plays in each category to the size of the original dataframe of plays.\n",
        "    - Goal is to make sure the number of plays is the same."
      ],
      "metadata": {
        "id": "F2OYnxdNecCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorized plays\n",
        "\n",
        "plays_list = [df_2023_pass_week2,         # Offense\n",
        "              df_2023_run_week2,\n",
        "              df_2023_interception_week2, # Defense\n",
        "              df_2023_sack_week2,\n",
        "              df_2023_punt_week2,         # Special Teams\n",
        "              df_2023_kickoff_week2,\n",
        "              df_2023_touchdown_week2,    # Scoring\n",
        "              df_2023_extrapoint_week2,\n",
        "              df_2023_fieldgoal_week2,\n",
        "              df_2023_2pt_conversion_week2,\n",
        "              df_2023_fumble_week2,       # Other\n",
        "              df_2023_penalty_week2,\n",
        "              df_2023_turnover_on_downs_week2,\n",
        "              df_2023_timeout_week2]\n",
        "\n",
        "num_plays_categorized = 0\n",
        "\n",
        "for plays in plays_list:\n",
        "  num_plays_categorized = num_plays_categorized + len(plays)\n",
        "\n",
        "num_plays_categorized == len(week2_2023_plays)"
      ],
      "metadata": {
        "id": "PrHB0TVqezXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PIPELINE\n",
        "- ORDER\n",
        "  1. Team Dictionary\n",
        "    - Used to map team names with their acronyms\n",
        "  2. Regular expressions\n",
        "    - Used to find common patterns within raw data\n",
        "  3. Transforming Data\n",
        "    - So far, only label encoding\n",
        "  4. Cleaning methods\n",
        "    - Unique cleaning methods for each play type\n",
        "  5. Main pipeline method\n",
        "    - Control flow of cleaning methods"
      ],
      "metadata": {
        "id": "DsEqD2k9pCRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. TEAM DICTIONARY"
      ],
      "metadata": {
        "id": "mXMEOEkUrQ9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KEY: Team name\n",
        "# VALUE: Acronym of team\n",
        "\n",
        "dict_teams = {\n",
        "    'Cardinals': 'ARI', 'Falcons': 'ATL', 'Ravens': 'BAL', 'Bills': 'BUF', 'Panthers': 'CAR', 'Bears': 'CHI',\n",
        "    'Bengals': 'CIN', 'Browns': 'CLE', 'Cowboys': 'DAL', 'Broncos': 'DEN', 'Lions': 'DET', 'Packers': 'GB',\n",
        "    'Texans': 'HOU', 'Colts': 'IND', 'Jaguars': 'JAX', 'Chiefs': 'KC', 'Raiders': 'LV', 'Chargers': 'LAC',\n",
        "    'Rams': 'LAR', 'Dolphins': 'MIA', 'Vikings': 'MIN', 'Patriots': 'NE', 'Saints': 'NO', 'Giants': 'NYG',\n",
        "    'Jets': 'NYJ', 'Eagles': 'PHI', 'Steelers': 'PIT', '49ers': 'SF', 'Seahawks': 'SEA', 'Buccaneers': 'TB',\n",
        "    'Titans': 'TEN', 'Commanders': 'WAS'\n",
        "}"
      ],
      "metadata": {
        "id": "xBwDL0psvqtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KEY: Full Team name\n",
        "# VALUE: Acronym of team\n",
        "\n",
        "dict_teams_2 = {\n",
        "    'Arizona Cardinals': 'ARI', 'Atlanta Falcons': 'ATL', 'Baltimore Ravens': 'BAL', 'Buffalo Bills': 'BUF', 'Carolina Panthers': 'CAR', 'Chicago Bears': 'CHI',\n",
        "    'Cincinnati Bengals': 'CIN', 'Cleveland Browns': 'CLE', 'Dallas Cowboys': 'DAL', 'Denver Broncos': 'DEN', 'Detroit Lions': 'DET', 'Green Bay Packers': 'GB',\n",
        "    'Houston Texans': 'HOU', 'Indianapolis Colts': 'IND', 'Jacksonville Jaguars': 'JAX', 'Kansas City Chiefs': 'KC', 'Las Vegas Raiders': 'LV', 'Los Angeles Chargers': 'LAC',\n",
        "    'Los Angeles Rams': 'LAR', 'Miami Dolphins': 'MIA', 'Minnesota Vikings': 'MIN', 'New England Patriots': 'NE', 'New Orleans Saints': 'NO', 'New York Giants': 'NYG',\n",
        "    'New York Jets': 'NYJ', 'Philadelphia Eagles': 'PHI', 'Pittsburgh Steelers': 'PIT', 'San Francisco 49ers': 'SF', 'Seattle Seahawks': 'SEA', 'Tampa Bay Buccaneers': 'TB',\n",
        "    'Tennessee Titans': 'TEN', 'Washington Commanders': 'WAS'\n",
        "}"
      ],
      "metadata": {
        "id": "gZlpKH7PrSdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KEY: Acronym of team\n",
        "# VALUE: Team name\n",
        "\n",
        "dict_teams_3 = {\n",
        "    'ARI': 'Arizona Cardinals', 'ATL': 'Atlanta Falcons', 'BAL': 'Baltimore Ravens', 'BUF': 'Buffalo Bills', 'CAR': 'Carolina Panthers', 'CHI': 'Chicago Bears',\n",
        "    'CIN': 'Cincinnati Bengals', 'CLE': 'Cleveland Browns', 'DAL': 'Dallas Cowboys', 'DEN': 'Denver Broncos', 'DET': 'Detroit Lions', 'GB': 'Green Bay Packers',\n",
        "    'HOU': 'Houston Texans', 'IND': 'Indianapolis Colts', 'JAX': 'Jacksonville Jaguars', 'KC': 'Kansas City Chiefs', 'LV': 'Las Vegas Raiders', 'LAC': 'Los Angeles Chargers',\n",
        "    'LAR': 'Los Angeles Rams', 'MIA': 'Miami Dolphins', 'MIN': 'Minnesota Vikings', 'NE': 'New England Patriots', 'NO': 'New Orleans Saints', 'NYG': 'New York Giants',\n",
        "    'NYJ': 'New York Jets', 'PHI': 'Philadelphia Eagles', 'PIT': 'Pittsburgh Steelers', 'SF': 'San Francisco 49ers', 'SEA': 'Seattle Seahawks', 'TB': 'Tampa Bay Buccaneers',\n",
        "    'TEN': 'Tennessee Titans', 'WAS': 'Washington Commanders'\n",
        "}"
      ],
      "metadata": {
        "id": "EXmd10nzuUSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. REGULAR EXPRESSIONS"
      ],
      "metadata": {
        "id": "rxluYvbU0t_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################\n",
        "# REGULAR EXPRESSIONS USED TO LOCATE SPECIFIC DATA #\n",
        "####################################################\n",
        "\n",
        "###########\n",
        "# GENERAL #\n",
        "###########\n",
        "\n",
        "# Players name (Grabs every variation come across so far)\n",
        "# - I need this to be able to grab 'A.St. Brown' & 'C.Edwards-Helaire' & 'L.Van Ness'\n",
        "name_pattern = r\"(?:[A-Z][a-z]{0,4}\\.)+(?:[- ]?[A-Z][a-z]+)+\"\n",
        "\n",
        "spotting_pattern = \"(?:([A-Z]+) )?(-?[0-9]+)\"\n",
        "\n",
        "# Injuries (Returns the player(s) who go injuried during play)\n",
        "injury_pattern = f\"[A-Z]+-({name_pattern}) was injured during the play\"\n",
        "\n",
        "# Touchdowns\n",
        "touchdown_pattern = f\"for ([0-9]+) yards?, TOUCHDOWN\"\n",
        "\n",
        "################\n",
        "# PLAY DETAILS #\n",
        "################\n",
        "\n",
        "# Positioning at the end of the play\n",
        "standard_play_end_pattern = \"(?:to|at) (?:([A-Z]+) )?([0-9]+) for (no gain|-?[0-9]+)(?: yards?)?\"\n",
        "\n",
        "###########\n",
        "# OFFENSE #\n",
        "###########\n",
        "\n",
        "# Passer (Player passing, Player spiking, Player who got sacked)\n",
        "passer_name_pattern = f\"({name_pattern}) (?:pass|spiked|sacked)\"\n",
        "\n",
        "# Pass play (Returns intended receiver and the direction of the pass)\n",
        "receiver_pattern = f\"(short|deep) (left|right|middle) (?:to|intended for) ({name_pattern})\"\n",
        "\n",
        "# Rushing play (Player running ball)\n",
        "rusher_pattern = f\"({name_pattern})(?: scrambles)? (?:(left|right|up|kneels)) (?:(the middle|guard|tackle|end))?\"\n",
        "\n",
        "# 2 Point Conversion (Pass attempt)\n",
        "tp_conversion_pass_pattern = f\"({name_pattern}) pass to ({name_pattern})\"\n",
        "\n",
        "# 2 Point Conversion (Rush attempt)\n",
        "tp_conversion_rush_pattern = f\"({name_pattern}) rushes (left|right|up) (the middle|guard|tackle|end)\"\n",
        "\n",
        "###########\n",
        "# DEFENSE #\n",
        "###########\n",
        "\n",
        "# Tackles\n",
        "\n",
        "# solo / sack\n",
        "solo_tackle_pattern = rf\"\\(({name_pattern})\\)\"\n",
        "\n",
        "# shared\n",
        "shared_tackle_pattern = rf\"\\(({name_pattern}), ({name_pattern})\\)\"\n",
        "\n",
        "# shared\n",
        "assisted_tackle_pattern = rf\"\\(({name_pattern}); ({name_pattern})\\)\"\n",
        "\n",
        "# Pressure (Who applied pressure to passer)\n",
        "# - I think it might be possible for multiple defenders to apply pressure to the passer.\n",
        "defense_pressure_name_pattern = rf\"\\[({name_pattern})\\]\"\n",
        "\n",
        "# Split sack (Players who equally received credit for sack)\n",
        "split_sack_pattern = f\"sack split by ({name_pattern}) and ({name_pattern})\"\n",
        "\n",
        "# Defense takeaway (takeaway for yardage)\n",
        "# D.Hill pushed ob at 50 for 20 yards (J.Wills)\n",
        "# J.Bates to ATL 49 for no gain (T.Marshall)\n",
        "defensive_takeaway_run_pattern = f\"({name_pattern}) (?:pushed ob at|ran ob at|to)(?: ([A-Z]+))? (-?[0-9]+) for (no gain|-?[0-9]+)(?: yards?)?\" # yardage after fumble recovery & yardage after interception\n",
        "\n",
        "# Interception (Player who intercepted pass)\n",
        "interception_name_pattern = rf\"INTERCEPTED by ({name_pattern})(?:[ \\t]*(?:\\({name_pattern}\\)|\\[{name_pattern}\\]))* at ((?:[A-Z]+ )?[0-9]+)\"\n",
        "\n",
        "\n",
        "#################\n",
        "# SPECIAL TEAMS #\n",
        "#################\n",
        "\n",
        "# Punting play (Who was the punter, How many yards the ball went, Who was the Longsnapper)\n",
        "punting_pattern = f\"({name_pattern}) punts (-?[0-9]+) yards? to(?: ((?:[A-Z]+ )?-?[0-9]+)| end zone), Center-({name_pattern})\"\n",
        "\n",
        "# Punt return resulting in fair catch\n",
        "punt_fair_catch_pattern = f\", fair catch by ({name_pattern})\"\n",
        "\n",
        "# Punt or kickoff downed by\n",
        "# downed by PHI-S.Brown\n",
        "kick_downed_by_pattern = f\"downed by [A-Z]+-({name_pattern})\"\n",
        "\n",
        "# Kickoff play (Who was the kicker, How many yards the ball was kicked )\n",
        "kickoff_pattern = f\"({name_pattern}) kicks(?: onside)? (-?[0-9]+) yards from ((?:[A-Z]+ )?[0-9]+) to ((?:[A-Z]+ )?-?[0-9]+|end zone)\"\n",
        "\n",
        "# Field goal (Good OR No Good)\n",
        "field_goal_pattern = f\"({name_pattern}) (-?[0-9]+) yard field goal is (?:GOOD|No Good),(?: ([A-Za-z]+(?: [A-Za-z]+)*),)? Center-({name_pattern}), Holder-({name_pattern}).\"\n",
        "\n",
        "# Field goal (Blocked)\n",
        "# â€” C.McLaughlin 40 yard field goal is BLOCKED (R.Green), Center-Z.Triner, Holder-J.Camarda, recovered by TB-J.Camarda at 50.\n",
        "field_goal_blocked_pattern = rf\"({name_pattern}) (-?[0-9]+) yard field goal is BLOCKED \\(({name_pattern})\\), Center-({name_pattern}), Holder-({name_pattern}), (?:RECOVERED|recovered) by ([A-Z]+)-({name_pattern}) at ((?:[A-Z]+ )?[0-9]+)\"\n",
        "\n",
        "# Extra Point (Good OR No Good)\n",
        "extra_point_pattern = f\"({name_pattern}) extra point is (?:GOOD|No Good),(?: ([A-Za-z]+(?: [A-Za-z]+)*),)? Center-({name_pattern}), Holder-({name_pattern}).\""
      ],
      "metadata": {
        "id": "0VNajl9Y0zNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. TRANSFORMING DATA"
      ],
      "metadata": {
        "id": "7CF2Mdx00_6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PURPOSE:\n",
        "# - Take value for 'PlayTimeFormation' and split into 3 separate features.\n",
        "#   1. GameClock (Will come about when renaming 'PlayTimeFormation')\n",
        "#   2. Quarter (This feature already exists, the values within 'PlayTimeFormation' are more accurate and will replace the value in here originaly)\n",
        "#   3. Formation\n",
        "\n",
        "def playtimeformation_split(df_plays):\n",
        "\n",
        "  df_plays_copy = df_plays.copy()\n",
        "\n",
        "  new_columns = ['Formation']\n",
        "\n",
        "  df_plays_copy = df_plays_copy.rename(columns = {'PlayTimeFormation': 'GameClock'})\n",
        "\n",
        "  df_plays = df_plays.reindex(columns=df_plays.columns.tolist() + new_columns)\n",
        "\n",
        "  # Splitting original feauture 'PlayTimeFormation' (Now known as 'TimeLeftInQuarter')\n",
        "  for idx, play in df_plays_copy['GameClock'].items():\n",
        "    value_elements = play.split(' ')\n",
        "    # Some plays (e.g. Kickoff) will only have the formation as a value\n",
        "    if len(value_elements) <= 1:\n",
        "      df_plays_copy.at[idx, 'Formation'] = value_elements[0]\n",
        "      df_plays_copy.at[idx, 'GameClock'] = \"\"\n",
        "    else:\n",
        "      df_plays_copy.at[idx, 'GameClock'] = value_elements[0]\n",
        "      df_plays_copy.at[idx, 'Quarter'] = value_elements[1]\n",
        "      df_plays_copy.at[idx, 'Formation'] = \" \".join(value_elements[2::])\n",
        "\n",
        "  # Transform values in 'Quarter' feature from string to integer (e.g. '1st Quarter' -> 1)\n",
        "  dict_replace_quarter = {'1st Quarter': 1, '2nd Quarter': 2, '3rd Quarter': 3, '4th Quarter': 4,\n",
        "                          '1st': 1, '2nd': 2, '3rd': 3, '4th': 4}\n",
        "\n",
        "  # All overtime quarters will be have the value 5 in their place\n",
        "  df_plays_copy['Quarter'] = df_plays_copy['Quarter'].map(dict_replace_quarter).fillna(5).astype(int)\n",
        "\n",
        "  return df_plays_copy\n",
        "\n",
        "# PURPOSE:\n",
        "# - Take value for 'PlayStart' and split into 2 separate features.\n",
        "#   1. DownAndDistance (Will come about when renaming 'PlayStart')\n",
        "#   2. FieldPosition (Start of play)\n",
        "\n",
        "def playstart_split(df_plays):\n",
        "\n",
        "  df_plays_copy = df_plays.copy()\n",
        "\n",
        "  new_columns = ['FieldPosition']\n",
        "\n",
        "  df_plays_copy = df_plays_copy.rename(columns = {'PlayStart': 'DownAndDistance'})\n",
        "\n",
        "  df_plays_copy = df_plays_copy.reindex(columns=df_plays_copy.columns.tolist() + new_columns)\n",
        "\n",
        "  df_plays_copy['FieldPosition'] = df_plays_copy['FieldPosition'].astype(str)\n",
        "\n",
        "  # Splitting original feature 'PlayStart' (Now known as 'DownAndDistance')\n",
        "  for idx, play in df_plays_copy['DownAndDistance'].items():\n",
        "    # Some plays to not have a down and distance or field position and contain 'nan' values here,\n",
        "    # this is to catcht those plays and keep going. (e.g. Kickoff / Extra Point / etc..)\n",
        "    if pd.isna(play):\n",
        "      continue\n",
        "    else:\n",
        "      value_elements = play.split(' at ')\n",
        "      df_plays_copy.at[idx, 'DownAndDistance'] = value_elements[0]\n",
        "      df_plays_copy.at[idx, 'FieldPosition'] = value_elements[1]\n",
        "\n",
        "  return df_plays_copy\n",
        "\n",
        "# PURPOSE:\n",
        "# - Keep consistence with team names\n",
        "#   - A team name will always be represented by their acronym\n",
        "\n",
        "def consistent_team_names(df_plays):\n",
        "\n",
        "  df_plays_copy = df_plays.copy()\n",
        "\n",
        "  df_plays_copy['AwayTeam'] = df_plays_copy['AwayTeam'].map(dict_teams)\n",
        "  df_plays_copy['HomeTeam'] = df_plays_copy['HomeTeam'].map(dict_teams)\n",
        "  df_plays_copy['TeamWithPossession'] = df_plays_copy['TeamWithPossession'].map(dict_teams_2)\n",
        "\n",
        "  return df_plays_copy"
      ],
      "metadata": {
        "id": "5hNOLgeqBZ_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. CLEANING METHODS"
      ],
      "metadata": {
        "id": "HZYDXAFyFxAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HELPER CLEANING METHODS"
      ],
      "metadata": {
        "id": "ISmCYevIwYLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SPLIT PLAY DESCRIPTION INTO SENTENCES"
      ],
      "metadata": {
        "id": "axDkGMWkNFut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PURPOSE:\n",
        "# - Function will split the feature \"PlayDescription\" into\n",
        "# its individual sentences and place them in a list.\n",
        "\n",
        "# - I am using playdescription.split(\". \") to separate\n",
        "#   sentences within play description. The problem here\n",
        "#   is that sometimes a player will have \". \" within their\n",
        "#   name, causing a sentence to split into 2 with the\n",
        "#   divide being in the middle of the players name. To\n",
        "#   overcome this, I will replace the \". \" character\n",
        "#   combination within player names with the string\n",
        "#   \"<DOT>\" then split play description into separate\n",
        "#   sentences and revert the player names back to normal\n",
        "#   after the split.\n",
        "\n",
        "def split_play_description(play_description):\n",
        "\n",
        "  # Finding all player names that were mentioned in the play\n",
        "  player_names = re.findall(name_pattern, play_description)\n",
        "\n",
        "  # Creating map for player names that have \". \" within their name\n",
        "  # and mapping them to a safe replacement name for the time being.\n",
        "  replacements = {}\n",
        "  for name in player_names:\n",
        "    if \". \" in name:\n",
        "      protected_name = name.replace(\". \", \"<DOT>\")\n",
        "      replacements[name] = protected_name\n",
        "\n",
        "  # Replacing original player name with safe replacement name in\n",
        "  # play description\n",
        "  for original, protected in replacements.items():\n",
        "    play_description = play_description.replace(original, protected)\n",
        "\n",
        "  # Splitting play description by \". \"\n",
        "  play_split = play_description.split(\". \")\n",
        "\n",
        "  # Revert player names back to normal in play_split\n",
        "  restored_names = [s.replace(\"<DOT>\", \". \") for s in play_split]\n",
        "\n",
        "  return restored_names"
      ],
      "metadata": {
        "id": "JtRp81IwND1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### YARDAGE BETWEEN SPOTTINGS"
      ],
      "metadata": {
        "id": "lcFfDZuUwfXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PURPOSE:\n",
        "# - Calculate the yardage between two spottings\n",
        "\n",
        "\n",
        "\n",
        "# MOST BENEFICIAL WHERE:\n",
        "# 1. fumbled plays\n",
        "# 2. penalty plays\n",
        "\n",
        "# CONCERNS\n",
        "# 1. Should I only use this method for plays that absolutely need it?\n",
        "#    - This seems like it would be a lengthy process having to go through\n",
        "#      this method for each play.\n",
        "\n",
        "# WHAT I NEED\n",
        "# 1. start spotting\n",
        "# 2. end spotting\n",
        "# 3. direction to goal\n",
        "\n",
        "# FEATURES THAT COULD HELP:\n",
        "\n",
        "# - STRICTLY FOR DIRECITON\n",
        "#   1. dataframe of plays (NOT IMPLEMENTED IN FIRST ITERATION)\n",
        "#   2. play index (NOT IMPLEMENTED IN FIRST ITERATION)\n",
        "#      - might need to reference other plays in the drive or quarter\n",
        "#      DESIGN NOTE:\n",
        "#      - The index does not have to be the original from the dataframe of plays,\n",
        "#        the dataframe of plays does have to be original. I just need to be able\n",
        "#        to grab features from this play being looked at to reference other plays\n",
        "#        within the dataframe of plays.\n",
        "\n",
        "# - BREAD AND BUTTER (most will only need these)\n",
        "#   3. description of action within play (could be a slice of a single play)\n",
        "#      - This is where I will find the 'end spotting'.\n",
        "#      - Some plays will have multiple actions with different yardage gains in them.\n",
        "#        I need to pinpoint which action I am looking at specifically\n",
        "#   4. start spotting\n",
        "#      - Because of the multiple actions nature of some of these plays\n",
        "#        (fumbles / penalties) I will need to locate the start spotting before\n",
        "#        hand.\n",
        "#      DESIGN NOTE:\n",
        "#      - I may have to cycle regular expressions to find the correct end spotting\n",
        "\n",
        "# DESIGN MENTALITY:\n",
        "# - Iterate over time.\n",
        "\n",
        "def yardage_between_spottings(df_plays, play_index, start_spotting, description_with_end_spotting):\n",
        "\n",
        "  # DIRECTION\n",
        "  # - I need to figure out which zone is past the 50 and which zone is within the 50 for the team\n",
        "  #   with the ball. (e.g. 'BUF' is 100-51, 'KC' is 49-0, 50 is neutral)\n",
        "  #   - I will find this by looking at\n",
        "  #     1. start spotting\n",
        "  #     2. end spotting\n",
        "  #     3. yardage gained between\n",
        "  #        - Majority of play descriptions will have the 'end spotting' and\n",
        "  #          'yardage gained between'. These are essential and if they are not\n",
        "  #          located within the passed in 'description_with_end_spotting' then\n",
        "  #          that is when I will need to look at another play within this quarter.\n",
        "\n",
        "  # DESIGN\n",
        "  # - Every spotting will have both the zone and the yardage (e.g. 'BUF 20')\n",
        "  #   - I want all spottings to be on a 100 point scale to represent the length of\n",
        "  #     the field, the zone will aid in this.\n",
        "  #     - EXAMPLE:\n",
        "  #       - 'BUF 20'\n",
        "  #       - (BUF zone is 100-51)\n",
        "  #         - 100 - 20 = 80 yards to endzone\n",
        "  #       - (BUF zone is 49-0)\n",
        "  #         - 20 yard to endzone\n",
        "  #   - The reason for doing this is so that I will be able to tell, given 2 spottings,\n",
        "  #     whether it was a negative gain vs positive.\n",
        "  #     - EXAMPLE:\n",
        "  #       - start_spotting = BUF 20\n",
        "  #       - end_spotting   = BUF 30\n",
        "  #       - (BUF zone is 100-51)\n",
        "  #         - start_spotting = 100 - 20 = 80 yards until endzone\n",
        "  #         - end_spotting   = 100 - 30 = 70 yards until endzone\n",
        "  #           - yardage gained = 80 - 70 = 10 yards gained\n",
        "  #       - start_spotting = BUF 20\n",
        "  #       - end_spotting   = BUF 30\n",
        "  #       - (BUF zone is 49-0)\n",
        "  #         - start_spotting = 20 yards until endzone\n",
        "  #         - end_spotting   = 30 yards until endzone\n",
        "  #           - yardage gained = 20 - 30 = -10 yards gained\n",
        "\n",
        "  # LOCATE\n",
        "  # start_territory\n",
        "  # start_yardage\n",
        "  # end_territory\n",
        "  # end_yardage\n",
        "  # pseudo_play_yardage\n",
        "\n",
        "  ##################\n",
        "  # START SPOTTING #\n",
        "  ##################\n",
        "\n",
        "  # Splitting start_spotting (e.x. [['BUF'], ['20']])\n",
        "  start_elements = re.findall(spotting_pattern, start_spotting)\n",
        "  start_territory = start_elements[0][0]\n",
        "  start_yardage = int(start_elements[0][1])\n",
        "  # print(start_territory)\n",
        "  # print(start_yardage)\n",
        "\n",
        "  ##########################################\n",
        "  # END SPOTTING AND PSUEDO YARDAGE GAINED #\n",
        "  ##########################################\n",
        "  # - end_territory\n",
        "  # - end_yardage\n",
        "  # - psuedo_play_yardage\n",
        "\n",
        "  # (e.x. [['BUF'], ['30'], ['10']])\n",
        "  end_spotting_and_play_yardage = re.findall(standard_play_end_pattern, description_with_end_spotting)\n",
        "  # (e.x. ['5'])\n",
        "  touchdown = re.findall(touchdown_pattern, description_with_end_spotting)\n",
        "  # STANDARD\n",
        "  if end_spotting_and_play_yardage:\n",
        "    end_territory = end_spotting_and_play_yardage[0][0]\n",
        "    end_yardage = int(end_spotting_and_play_yardage[0][1])\n",
        "    # Grabbing yardage from play description\n",
        "    if end_spotting_and_play_yardage[0][2] == 'no gain':\n",
        "      return 0\n",
        "    else:\n",
        "      pseudo_play_yardage = int(end_spotting_and_play_yardage[0][2])\n",
        "  # TOUCHDOWN\n",
        "  elif touchdown:\n",
        "    pseudo_play_yardage = int(touchdown[0])\n",
        "    # Same zone touchdown\n",
        "    if int(touchdown[0]) < 50:\n",
        "      end_territory = start_territory\n",
        "    # Opposite zone touchdown\n",
        "    if int(touchdown[0]) > 50:\n",
        "      # touchdown was in opposite zone as start zone\n",
        "      if df_plays['HomeTeam'].loc[play_index] == start_territory:\n",
        "        end_territory = df_plays['AwayTeam'].loc[play_index]\n",
        "      else:\n",
        "        end_territory = df_plays['HomeTeam'].loc[play_index]\n",
        "    # 50 yard line\n",
        "    else:\n",
        "      end_territory = None\n",
        "    end_yardage = 50\n",
        "  # PLAY FAILED (e.g. pass incomplete)\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "  #######################\n",
        "  # CALCULATING YARDAGE #\n",
        "  #######################\n",
        "\n",
        "  # PLAN ON HOW TO LOCATE ZONES\n",
        "  # 1. spotting_difference\n",
        "  #    - ( start_spotting - end_spotting )\n",
        "  # 2. pseudo_play_yardage\n",
        "  #    - Was the yardage recorded in the play\n",
        "  #      description positive or negative?\n",
        "\n",
        "  # SCHEMATIC? BLUEPRINT? I cant figure out the right word.\n",
        "  # Standard cases (start position and end position are in the same zone):\n",
        "  # spotting_difference (+) & pseudo_play_yardage (+):\n",
        "  # - the start position team zone (49-0)\n",
        "  # spotting_difference (-) & pseudo_play_yardage (+):\n",
        "  # - the start position team zone (100-51)\n",
        "  # spotting_difference (+) & pseudo_play_yardage (-):\n",
        "  # - the start position team zone (100-51)\n",
        "  # spotting_difference (-) & pseudo_play_yardage (-):\n",
        "  # - the start position team zone (49-0)\n",
        "\n",
        "  # Unique cass (start position and ending position are in different zones):\n",
        "  # zones switch (e.g. KC 47 -> BUF 47)\n",
        "  # pseudo_play_yardage (+):\n",
        "  # - the start position team zone (100-51)\n",
        "  # pseudo_play_yardage (-):\n",
        "  # - the start position team zone (49-0)\n",
        "\n",
        "  # Standard cases\n",
        "  if (start_territory == end_territory):\n",
        "    # spotting_difference (+)\n",
        "    if start_yardage > end_yardage:\n",
        "      # pseudo_play_yardage (+)\n",
        "      # starting position 49-0 zone\n",
        "      if pseudo_play_yardage > 0:\n",
        "        starting_position = start_yardage\n",
        "        ending_position = end_yardage\n",
        "      # pseudo_play_yardage (-)\n",
        "      # starting position 100-51\n",
        "      else:\n",
        "        starting_position = 100 - start_yardage\n",
        "        ending_position = 100 - end_yardage\n",
        "    # spotting_difference (-)\n",
        "    else:\n",
        "      # pseudo_play_yardage (+)\n",
        "      # starting position 100-51\n",
        "      if pseudo_play_yardage > 0:\n",
        "        starting_position = 100 - start_yardage\n",
        "        ending_position = 100 - end_yardage\n",
        "      # pseudo_play_yardage (-)\n",
        "      # starting position 49-0\n",
        "      else:\n",
        "        starting_position = start_yardage\n",
        "        ending_position = end_yardage\n",
        "  else:\n",
        "    # pseudo_play_yardage (+)\n",
        "    # starting position 100-51\n",
        "    if pseudo_play_yardage > 0:\n",
        "      starting_position = 100 - start_yardage\n",
        "      ending_position = end_yardage\n",
        "    # pseudo_play_yardage (-)\n",
        "    # starting position 49-0\n",
        "    else:\n",
        "      starting_position = start_yardage\n",
        "      ending_position = 100 - end_yardage\n",
        "\n",
        "\n",
        "  # # DESIGN CHECK. (Checking for accuracy)\n",
        "  # if pseudo_play_yardage != int(starting_position) - int(ending_position):\n",
        "  #   print(pseudo_play_yardage)\n",
        "  #   print(int(starting_position) - int(ending_position))\n",
        "  #   raise ValueError(f\"Yardage mismatch at play_index {play_index}, \\\"{description_with_end_spotting}\\\"\")\n",
        "\n",
        "  return int(starting_position) - int(ending_position)"
      ],
      "metadata": {
        "id": "hyZazzsewbsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FUMBLES"
      ],
      "metadata": {
        "id": "c4Q_lOfDWS5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PURPOSE:\n",
        "# - A method that will clean fumbled plays for every play type\n",
        "\n",
        "# INPUT:\n",
        "# - Dataframe of plays & index of fumbled play\n",
        "\n",
        "# OUTPUT:\n",
        "# - Potential multi-row dataframe that will contain every action (possession)\n",
        "#   that occured during the play. (e.g. rush/fumble recovery for yards/etc..)\n",
        "\n",
        "# DESIGN IDEA (STEP BY STEP):\n",
        "# 1. Receive single row dataframe of fumble play\n",
        "# 2. Split play description into separate sentences\n",
        "# 3. Group split sentences in a way where each grouping contains all information\n",
        "#    needed to complete a single row in the return dataframe.\n",
        "#    - There are cases where a single sentence will have information that is\n",
        "#      needed for multiple rows.\n",
        "#      EXAMPLE:\n",
        "#      - \"FUMBLES (D.White) [D.White], RECOVERED by TB-C.Izien at CHI 48\"\n",
        "#        - This sentence contains:\n",
        "#          1. Who forced the fumble ................ '(D.White)'\n",
        "#          2. Who applied pressure..? .............. '[D.White]'\n",
        "#          3. What team recovered the fumble ....... 'TB-'\n",
        "#          4. Who recovered the fumble ............. '-C.Izien'\n",
        "#          5. The spotting of the fumble recovery .. 'CHI 48'\n",
        "#          - The sentence before this one was some sort of action or play\n",
        " #           (run/pass/etc..)\n",
        "#            - ROW 1\n",
        "#              - Will need:\n",
        "#                1. Who forced the fumble ................ '(D.White)'\n",
        "#                2. Who applied pressure..? .............. '[D.White]'\n",
        "#                3. What team recovered the fumble ....... 'TB-'\n",
        "#                4. Who recovered the fumble ............. '-C.Izien'\n",
        "#                5. The spotting of the fumble recovery .. 'CHI 48'\n",
        "#          - The sentence after this could be a run after recovery\n",
        "#            - ROW 2\n",
        "#              - Will need:\n",
        "#                5. The spotting of the fumble recovery .. 'CHI 48'\n",
        "#                   - For the start spotting of the run after recovery\n",
        "# 4. Clean grouped sentences into accurate and useable data\n",
        "#    - Each grouping of sentences will be a row in the return dataframe\n",
        "# 5. Return the new clean fumbled play dataframe\n",
        "\n",
        "# THOUGHTS:\n",
        "# 1 - I THINK that the initial player who fumbled and all players that follow\n",
        "#     are cleaned differently.\n",
        "#     - The yardage recorded by the initial player is a bit different than all\n",
        "#       players that fumble and recover after.\n",
        "#     - There is more than just this but this is all I can think of right now.\n",
        "# 2 - Rules for how yardage is recorded:\n",
        "#     - If a player fumbles and the person who recovers the fumble is:\n",
        "#       1. On the same team\n",
        "#          -> Yardage ends at the spotting of the recovery\n",
        "#       2. On the opposing team\n",
        "#          -> Yardage ends at the spotting of the fumble\n",
        "#     - If a player catches a pass\n",
        "#       -> fumbles\n",
        "#          -> recovers own fumble\n",
        "#             -> rushes for extra yards\n",
        "#                - Yardage for this player is from the LOS -> down\n",
        "#                - Yardage for the player who passed the ball if from\n",
        "#                  LOS -> down (Same as receiving yards from receiver)\n",
        "#                - Receiver is credited with a fumble\n",
        "#     - If a fumble occurs behind the LOS\n",
        "#       -> recovered by same team\n",
        "#          -> recovered behind LOS\n",
        "#             -> play is done\n",
        "#                = player that fumbled receives (-) yards\n",
        "#                  - Including all players who might have recovered and fumbled\n",
        "#                    before final player who recovered behind LOS.\n",
        "#             -> player rushes beyond LOS\n",
        "#                = initial player receives 0 yards\n",
        "#                  - Including all players who might have recovered and fumbled\n",
        "#                    before final player who crossed LOS.\n",
        "#        -> recovered beyond LOS\n",
        "#           = initial player receives 0 yards? <- double check this\n",
        "# 3 - Return dataframe format\n",
        "#     - For each row of the return dataframe, somewhere within one of the\n",
        "#       feauture values, I need to include a fraction that will show what row\n",
        "#       number the action is.\n",
        "# 4 - I need a summary row for certain playtypes\n",
        "#     - I forget for which ones, but I know I need it\n",
        "\n",
        "def helper_clean_fumble_play(df_plays, play_index):\n",
        "\n",
        "  print(play_index)\n",
        "\n",
        "  ###################################################\n",
        "  # 1. RECEIVE SINGLE ROW DATAFRAME OF FUMBLED PLAY #\n",
        "  ###################################################\n",
        "\n",
        "  # copy of original play row\n",
        "  df_play = df_plays.loc[play_index].copy()\n",
        "\n",
        "  #############################\n",
        "  # 2. SPLIT PLAY DESCRIPTION #\n",
        "  #############################\n",
        "\n",
        "  play_description = df_play['PlayDescription']\n",
        "\n",
        "  ############\n",
        "  # REVERSES #\n",
        "  ############\n",
        "  # In 'PlayDescription' all information before the \"reversed\" sentence is not needed.\n",
        "  # - All information before is stored within 'ReverseDetails' and the remaining is cleaned.\n",
        "  if play_description.find('REVERSED') != -1:\n",
        "    play_elements = split_play_description(play_description)\n",
        "    for i in play_elements:\n",
        "      if i.find(\"REVERSED\") != -1:\n",
        "        df_play['ReverseDetails'] = play_elements[:play_elements.index(i) + 1]\n",
        "        play_description = \". \".join(play_elements[play_elements.index(i) + 1:])\n",
        "        break\n",
        "\n",
        "  # Splitting 'PlayDescription' into a list of actions (sentences)\n",
        "  play_description_split = split_play_description(play_description)\n",
        "\n",
        "  play_description_length = \"- 1. PLAY DESCRIPTION -\"\n",
        "  print(\"-\" * len(play_description_length))\n",
        "  print(play_description_length)\n",
        "  print(\"-\" * len(play_description_length))\n",
        "  print(play_description)\n",
        "  print()\n",
        "\n",
        "  play_description_split_length = \"- 2. PLAY DESCRIPTION SPLIT -\"\n",
        "  print(\"-\" * len(play_description_split_length))\n",
        "  print(play_description_split_length)\n",
        "  print(\"-\" * len(play_description_split_length))\n",
        "  for i in play_description_split:\n",
        "    print(i)\n",
        "  print()\n",
        "\n",
        "  ############################\n",
        "  # 3. GROUP SPLIT SENTENCES #\n",
        "  ############################\n",
        "\n",
        "  # List of actions in play\n",
        "  # - Will be a 2D list (list of lists)\n",
        "  # - Every element within this list (each element is a list of its own) will\n",
        "  #   have a grouping of sentences that will represent a single action within\n",
        "  #   the play\n",
        "  play_split = []\n",
        "\n",
        "  # - I would like for each element in the list to have everything that it needs\n",
        "  #   to complete a row.\n",
        "  #   - This means that every element will have:\n",
        "  #     1. Start spotting (LOS or recovery spotting)\n",
        "  #     2. End spotting (down or fumble spotting)\n",
        "  #     3. Player with ball\n",
        "  #     4. Player who made tackle\n",
        "\n",
        "  # - I will organize each of these sentences by checking each one,\n",
        "  #   in cronological order, and determining whether the sentence:\n",
        "  #   1. Is the start of a new row (primary action)\n",
        "  #      - rushes/passes/etc..\n",
        "  #   2. Is an essential addition to a row (secondary action)\n",
        "  #       - fumble recoveries / etc..\n",
        "  #       - These sentences could also start a new row\n",
        "  #         - fumble recovery means a potential start to a new rush attempt\n",
        "  #           - this is part of having all information grouped together for\n",
        "  #             a single row\n",
        "\n",
        "  # Cycling through every action (sentence) within play\n",
        "  while (play_description_split):\n",
        "\n",
        "    # If a fumble recovery for action happens\n",
        "    action_after_fumble_recovery = False\n",
        "\n",
        "    #####################\n",
        "    # EXTRA INFORMATION #\n",
        "    #####################\n",
        "    # - Extra information that does not have to do with the play itself\n",
        "    #   (e.g. injuries/penalties/eligibility/etc..)\n",
        "\n",
        "    #####################\n",
        "    # SECONDARY ACTIONS #\n",
        "    #####################\n",
        "    # - The reason why this is checked first is because of formatting. For\n",
        "    #   fumble recoveries that result in an attempt to gain yards, I would like\n",
        "    #   for that sentence to be the start of a new element because it will\n",
        "    #   provide the start spotting for what comes next (rush/pass/etc..).\n",
        "\n",
        "    # - Center fumbled\n",
        "    #   - Could possibly take this sentence out completely, I do not think\n",
        "    #     there is any useful information in it.\n",
        "    #     - (e.g. \"P.Mahomes Aborted.\")\n",
        "    #       - P.Mahomes is not at fault for the fumble, his center is.\n",
        "    if ' aborted' in play_description_split[0].lower():\n",
        "      play_split.append([play_description_split[0]])\n",
        "      # - The reason for taking this out here is because it is not required\n",
        "      #   for any other row grouping.\n",
        "      play_description_split.pop(0)\n",
        "      continue\n",
        "\n",
        "    # - Sentence containing fumble description\n",
        "    if 'fumble' in play_description_split[0].lower():\n",
        "\n",
        "      # - Quarterback fumbled\n",
        "      if '(aborted)' in play_description_split[0].lower():\n",
        "        play_split.append([play_description_split[0]])\n",
        "      else:\n",
        "\n",
        "        # - Added onto element in list with primary action\n",
        "        play_split[len(play_split) - 1].append(play_description_split[0])\n",
        "\n",
        "      # - If the sentence is one that contains recovery information and is\n",
        "      #   followed by more actions, it will be the start of a new row\n",
        "      #   - This could be a problem in the future when there are penalties\n",
        "      #     and injuries.\n",
        "      if 'recover' in play_description_split[0].lower() and len(play_description_split) > 1:\n",
        "\n",
        "        # 1. Only grabbing the part of the sentence that has the recovery\n",
        "        #    information in it.\n",
        "        #    EXAMPLE:\n",
        "        #    - \"â€” P.Mahomes Aborted', 'C.Humphrey FUMBLES at KC 39, touched at\n",
        "        #         KC 38, recovered by KC-P.Mahomes at KC 40\"\n",
        "        #      - only want \"recovered by KC-P.Mahomes at KC 40\"\n",
        "        # 2. Set recovery info as start of new grouping for new row\n",
        "        # 3. Pop out recovery sentence, next sentence will be primary action\n",
        "        #    - (e.g. pass/run/etc..)\n",
        "        recover_info = play_description_split[0].split(\", \")\n",
        "        play_split.append([recover_info[len(recover_info) - 1]])\n",
        "        play_description_split.pop(0)\n",
        "\n",
        "        # - This is a fumble recovery for action, so the primary action will\n",
        "        #   need to append itself to this newly created element within the list\n",
        "        action_after_fumble_recovery = True\n",
        "\n",
        "    ###################\n",
        "    # PRIMARY ACTIONS #\n",
        "    ###################\n",
        "    # - Primary actions are sentences that have details about a player rushing\n",
        "    #   or passing the ball.\n",
        "\n",
        "    # - Sentence that contains information of an attempt to gain yards\n",
        "    for play_pattern in [passer_name_pattern, standard_play_end_pattern]:\n",
        "      if re.search(play_pattern, play_description_split[0]):\n",
        "        # - Attempt to gain yards after a fumble recovery\n",
        "        if action_after_fumble_recovery:\n",
        "          play_split[len(play_split) - 1].append(play_description_split[0])\n",
        "        # - Initial attempt to gain yards (Probably before fumble occured)\n",
        "        else:\n",
        "          play_split.append([play_description_split[0]])\n",
        "        break\n",
        "\n",
        "    # - Sentence should be taken care of, onto the next sentence in the play\n",
        "    play_description_split.pop(0)\n",
        "\n",
        "  play_description_grouped_length = \"- 3. PLAYDESCRIPTION GROUPED -\"\n",
        "  print(\"-\" * len(play_description_grouped_length))\n",
        "  print(play_description_grouped_length)\n",
        "  print(\"-\" * len(play_description_grouped_length))\n",
        "  for j in play_split:\n",
        "    print(j)\n",
        "  print()\n",
        "  print()\n",
        "\n",
        "  ##############################\n",
        "  # 4. CLEAN GROUPED SENTENCES #\n",
        "  ##############################\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "AhuZWjEvWimB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OFFENSIVE CLEANING METHODS"
      ],
      "metadata": {
        "id": "x_ZUQ4A8GADL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PASS PLAYS"
      ],
      "metadata": {
        "id": "leasyhVHHbV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PURPOSE:\n",
        "# - Clean all passing play types\n",
        "# INPUT PARAMETERS:\n",
        "# df_plays    - dataframe - NFL plays\n",
        "# index_start -  integer  - index in the dataframe of NFL plays where the method\n",
        "#                           will start cleaning in ascending order.\n",
        "# RETURN:\n",
        "# df_plays - dataframe - the same input df_plays but with all passing play types cleaned\n",
        "\n",
        "def clean_pass_plays(df_plays, index_start = None):\n",
        "\n",
        "  # Adjusting df_plays to start cleaning at a specified index (index_start)\n",
        "  if index_start != None:\n",
        "    # Locating all passing type plays (starting from 'index_start')\n",
        "    df_plays_adjusted = df_plays.loc[index_start:]\n",
        "    df_pass_plays = df_plays_adjusted[df_plays_adjusted['PlayOutcome'].str.contains('Pass')]\n",
        "  else:\n",
        "    # Locating all passing type plays (From entire input dataframe)\n",
        "    df_pass_plays = df_plays[df_plays['PlayOutcome'].str.contains('Pass')]\n",
        "\n",
        "  for idx, play in df_pass_plays['PlayDescription'].items():\n",
        "\n",
        "    # print(idx)\n",
        "    # print(play)\n",
        "\n",
        "    ################\n",
        "    # PLAY DETAILS #\n",
        "    ################\n",
        "\n",
        "    # Play Type\n",
        "    df_plays.loc[idx, 'PlayType'] = 'Pass'\n",
        "\n",
        "    ############\n",
        "    # REVERSES #\n",
        "    ############\n",
        "\n",
        "    # In 'PlayDescription' all information before the \"reversed\" sentence is not needed.\n",
        "    # - All information before the \"reversed\" sentence is stored within \"ReverseDetails\"\n",
        "    if play.find('REVERSED') != -1:\n",
        "      play_elements = play.split(\". \")\n",
        "      for i in play_elements:\n",
        "        if i.find('REVERSED') != -1:\n",
        "          df_plays.at[idx, 'ReverseDetails'] = play_elements[:play_elements.index(i) + 1]\n",
        "          play = \". \".join(play_elements[play_elements.index(i) + 1:])\n",
        "          break\n",
        "\n",
        "    ############################\n",
        "    # REPORTING IN AS ELIGIBLE #\n",
        "    ############################\n",
        "\n",
        "    # I do not think this contains any useful data so I am going to exclude it.\n",
        "    if play.find('reported in as eligible') != -1:\n",
        "      play_elements = play.split(\". \")\n",
        "      for i in play_elements:\n",
        "        if i.find('reported in as eligible') != -1:\n",
        "          play = \". \".join(play_elements[play_elements.index(i) + 1:])\n",
        "          break\n",
        "\n",
        "    ############\n",
        "    # LATERALS #\n",
        "    ############\n",
        "    # - Yardage gained from a lateral.. what would this look like?\n",
        "    #   - Would the lateral method completely clean that play?\n",
        "    #     - I think so.\n",
        "\n",
        "    ###########\n",
        "    # FUMBLES #\n",
        "    ###########\n",
        "    # - Yardage gained from a fumble.. what would this look like?\n",
        "    #   - Would the fumble method completele clean that play?\n",
        "    #     - I think so.\n",
        "    if play.lower().find(\"fumble\") != -1:\n",
        "      helper_clean_fumble_play(df_plays, idx)\n",
        "      continue\n",
        "\n",
        "    ###########\n",
        "    # OFFENSE #\n",
        "    ###########\n",
        "\n",
        "\n",
        "    # These may have to change in the future\n",
        "    # - I do not think that the value with the 'end_spotting' will always\n",
        "    #   be 'play'. I think that in the future, I will need to get more percise\n",
        "    #   with this.\n",
        "    #   - I think that the end spotting will actually always be in the description\n",
        "    #     somewhere. I will have to locate it\n",
        "    # - I do not think that 'start_spotting' will always be the field position.\n",
        "    start_spotting = df_plays.loc[idx, 'FieldPosition']\n",
        "    description_with_end_spotting = play\n",
        "    df_plays.loc[idx, 'Yardage'] = yardage_between_spottings(df_plays, idx, start_spotting, description_with_end_spotting)\n",
        "\n",
        "\n",
        "    # I am not giving up on this option of receiving play yardage\n",
        "    # VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\n",
        "    # - I think it is the fastest and is accurate for normal plays.\n",
        "\n",
        "    # action_yardage = re.findall(standard_play_end_pattern, play)\n",
        "    # if action_yardage:\n",
        "    #   print(action_yardage)\n",
        "    #   # End Spot\n",
        "    #   df_plays.loc[idx, 'EndSpot'] = \" \".join(action_yardage[0][:2])\n",
        "    #   # Yardage\n",
        "    #   if action_yardage[0][2] == 'no gain':\n",
        "    #     df_plays.loc[idx, 'Yardage'] = 0\n",
        "    #   else:\n",
        "    #     df_plays.loc[idx, 'Yardage'] = action_yardage[0][2]\n",
        "    # else:\n",
        "    #   print(\"No action yardage\")\n",
        "\n",
        "    # Passer\n",
        "    passer_name = re.findall(passer_name_pattern, play)\n",
        "    if passer_name:\n",
        "      # print(passer_name)\n",
        "      df_plays.loc[idx, 'Passer'] = passer_name[0]\n",
        "\n",
        "    # Receiver name and passing details\n",
        "    receiver_name_and_passing_details = re.findall(receiver_pattern, play)\n",
        "    if receiver_name_and_passing_details:\n",
        "      # print(receiver_name_and_passing_details)\n",
        "      df_plays.loc[idx, 'Direction'] = \" \".join(receiver_name_and_passing_details[0][:2])\n",
        "      df_plays.loc[idx, 'Receiver'] = receiver_name_and_passing_details[0][2]\n",
        "\n",
        "    # Unique situation (offense spikes the ball)\n",
        "    if play.find('spike') != -1:\n",
        "      df_plays.loc[idx, 'Direction'] = 'spiked' # Direction?\n",
        "\n",
        "    #############\n",
        "    #  DEFENSE  #\n",
        "    #############\n",
        "\n",
        "    solo_tackle = re.findall(solo_tackle_pattern, play)\n",
        "    if solo_tackle:\n",
        "      if df_plays.loc[idx, 'PlayDescription'].find('pass incomplete') != -1:\n",
        "        df_plays.loc[idx, 'PassDefendedBy'] = solo_tackle[0]\n",
        "      else:\n",
        "        df_plays.loc[idx, 'SoloTackle'] = solo_tackle[0]\n",
        "\n",
        "    shared_tackle = re.findall(shared_tackle_pattern, play)\n",
        "    if len(shared_tackle) > 0:\n",
        "      if df_plays.loc[idx, 'PlayDescription'].find('pass incomplete') != -1:\n",
        "        df_plays.at[idx, 'PassDefendedBy'] = shared_tackle[0]\n",
        "      else:\n",
        "        df_plays.at[idx, 'SharedTackle'] = shared_tackle[0]\n",
        "\n",
        "    assisted_tackle = re.findall(assisted_tackle_pattern, play)\n",
        "    if len(assisted_tackle) > 0:\n",
        "      df_plays.at[idx, 'AssistedTackle'] = assisted_tackle[0][::]\n",
        "\n",
        "    pressure_by = re.findall(defense_pressure_name_pattern, play)\n",
        "    if len(pressure_by) > 0:\n",
        "      df_plays.loc[idx, 'PressureBy'] = pressure_by[0]\n",
        "\n",
        "    ##############\n",
        "    #  INJURIES  #\n",
        "    ##############\n",
        "\n",
        "    injuries = re.findall(injury_pattern, play)\n",
        "    if len(injuries) > 0:\n",
        "      df_plays.at[idx, 'InjuredPlayers'] = injuries\n",
        "\n",
        "    # print()\n",
        "\n",
        "  if df_pass_plays.tail(1).index.tolist()[0] == idx:\n",
        "    return df_plays"
      ],
      "metadata": {
        "id": "lhG95DGqF67J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RUN PLAYS"
      ],
      "metadata": {
        "id": "dT8f80dqL8NV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PURPOSE:\n",
        "# - Clean run play types\n",
        "# INPUT PARAMETERS:\n",
        "# df_plays    - dataframe - dataframe of plays\n",
        "# index_start -  integer  - the starting index of the associated input dataframe\n",
        "#                           to begin cleaning.\n",
        "# RETURN:\n",
        "# df_plays - dataframe - dataframe of plays that now has all useful run play\n",
        "#                        data accessable and clean.\n",
        "\n",
        "# NOTE:\n",
        "# - This method will be used for all actions that involve running with the football.\n",
        "#   (e.g. fumble recoveries for yardage, fumble recoveries for touchdown, laterals, etc..)\n",
        "\n",
        "# def clean_run_plays(df_plays, index_start = None):\n",
        "def clean_run_plays(df_plays, start_spotting = None, index_start = None):\n",
        "\n",
        "  if index_start != None:\n",
        "    df_plays_adjusted = df_plays.loc[index_start:]\n",
        "    df_run_plays = df_plays_adjusted[df_plays_adjusted['PlayOutcome'].str.contains('Run')]\n",
        "  else:\n",
        "    df_run_plays = df_plays[df_plays['PlayOutcome'].str.contains('Run')]\n",
        "\n",
        "  # Iterating through every run play within 'df_run_plays'\n",
        "  for idx, play in df_run_plays['PlayDescription'].items():\n",
        "\n",
        "    # print(idx)\n",
        "    # print(play)\n",
        "\n",
        "    ################\n",
        "    # Play details #\n",
        "    ################\n",
        "\n",
        "    # Play Type\n",
        "    df_plays.loc[idx, 'PlayType'] = 'Run'\n",
        "\n",
        "    ############\n",
        "    # REVERSES #\n",
        "    ############\n",
        "\n",
        "    # In 'PlayDescription' all information before the \"reversed\" sentence is not needed.\n",
        "    # - All information before is stored within 'ReverseDetails' and the remaining is cleaned.\n",
        "    if play.find('REVERSED') != -1:\n",
        "      play_elements = play.split(\". \")\n",
        "      for i in play_elements:\n",
        "        if i.find(\"REVERSED\") != -1:\n",
        "          df_plays.at[idx, 'ReverseDetails'] = play_elements[:play_elements.index(i) + 1]\n",
        "          play = \". \".join(play_elements[play_elements.index(i) + 1:])\n",
        "          break\n",
        "\n",
        "    ############################\n",
        "    # REPORTING IN AS ELIGIBLE #\n",
        "    ############################\n",
        "\n",
        "    # I do not think this contains any useful data so I am going to exclude it.\n",
        "    if play.find('reported in as eligible') != -1:\n",
        "      play_elements = play.split(\". \")\n",
        "      for i in play_elements:\n",
        "        if i.find('reported in as eligible') != -1:\n",
        "          play = \". \".join(play_elements[play_elements.index(i) + 1:])\n",
        "          break\n",
        "\n",
        "    ############\n",
        "    # LATERALS #\n",
        "    ############\n",
        "    # - Yardage gained from a lateral.. what would this look like?\n",
        "    #   - Would the lateral method completely clean that play?\n",
        "    #     - I think so.\n",
        "\n",
        "    ###########\n",
        "    # FUMBLES #\n",
        "    ###########\n",
        "    # - Yardage gained from a fumble.. what would this look like?\n",
        "    #   - Would the fumble method completele clean that play?\n",
        "    #     - I think so.\n",
        "    if play.lower().find(\"fumble\") != -1:\n",
        "      helper_clean_fumble_play(df_plays, idx)\n",
        "      if df_run_plays.tail(1).index.tolist()[0] == idx:\n",
        "        return df_plays\n",
        "      # continue\n",
        "\n",
        "    #############\n",
        "    #  OFFENSE  #\n",
        "    #############\n",
        "\n",
        "    # Rusher\n",
        "    rusher_patterns = [rusher_pattern, defensive_takeaway_run_pattern]\n",
        "    # Loop through patterns and find the first match\n",
        "    for pattern in rusher_patterns:\n",
        "      rusher_name = re.findall(pattern, play)\n",
        "      if rusher_name:\n",
        "        # Regular run play\n",
        "        if rusher_patterns == rusher_pattern:\n",
        "          # Rusher\n",
        "          df_plays.loc[idx, 'Rusher'] = rusher_name[0][0]\n",
        "          # Direction\n",
        "          df_plays.at[idx, 'Direction'] = \" \".join(rusher_name[0][1::]).strip()\n",
        "        # Defensive takeaway (interception or fumble)\n",
        "        # - Because punt and kickoff returns follow the same format as\n",
        "        #   interception or fumble returns for yardage, this will also grab\n",
        "        #   the 'Returner' name for them.\n",
        "        else:\n",
        "          df_plays.loc[idx, 'Rusher'] = rusher_name[0][0]\n",
        "        break\n",
        "\n",
        "    # if not rusher_name:\n",
        "    #   raise ValueError(f\"rusher not found at {idx}, \\\"{play}\\\"\")\n",
        "\n",
        "    # These may have to change in the future\n",
        "    # - I do not think that the value with the 'end_spotting' will always\n",
        "    #   be 'play'. I think that in the future, I will need to get more percise\n",
        "    #   with this.\n",
        "    # - I do not think that 'start_spotting' will always be the field position.\n",
        "    # start_spotting = df_plays.loc[idx, 'FieldPosition']\n",
        "\n",
        "    if start_spotting == None:\n",
        "      start_spotting = df_plays.loc[idx, 'FieldPosition']\n",
        "    description_with_end_spotting = play\n",
        "    df_plays.loc[idx, 'Yardage'] = yardage_between_spottings(df_plays, idx, start_spotting, description_with_end_spotting)\n",
        "    # Need to reset for next play\n",
        "    start_spotting = None\n",
        "\n",
        "    # YARDAGE FOR HANDOFFS? #\n",
        "    # - That's what was here in the older version. Need to keep an eye out.\n",
        "\n",
        "    #############\n",
        "    #  DEFENSE  #\n",
        "    #############\n",
        "\n",
        "    solo_tackle = re.findall(solo_tackle_pattern, play)\n",
        "    if solo_tackle:\n",
        "        df_plays.loc[idx, 'SoloTackle'] = solo_tackle[0]\n",
        "\n",
        "    shared_tackle = re.findall(shared_tackle_pattern, play)\n",
        "    if len(shared_tackle) > 0:\n",
        "        df_plays.at[idx, 'SharedTackle'] = shared_tackle[0]\n",
        "\n",
        "    assisted_tackle = re.findall(assisted_tackle_pattern, play)\n",
        "    if len(assisted_tackle) > 0:\n",
        "      df_plays.at[idx, 'AssistedTackle'] = assisted_tackle[0][::]\n",
        "\n",
        "    ##############\n",
        "    #  INJURIES  #\n",
        "    ##############\n",
        "\n",
        "    injuries = re.findall(injury_pattern, play)\n",
        "    if len(injuries) > 0:\n",
        "      df_plays.at[idx, 'InjuredPlayers'] = injuries\n",
        "\n",
        "    #############\n",
        "    #  PENALTY  #\n",
        "    #############\n",
        "\n",
        "    # print()\n",
        "\n",
        "    # Return if the last play has been cleaned in 'df_run_plays'\n",
        "    if df_run_plays.tail(1).index.tolist()[0] == idx:\n",
        "      return df_plays"
      ],
      "metadata": {
        "id": "BM5FQkoaL3JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2PT CONVERSIONS"
      ],
      "metadata": {
        "id": "Worc0fONOglk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I NEED A LARGER SAMPLE SIZE FOR MORE PLAYS\n",
        "# - I need a sample size that has fumbled plays (if that's possible?)\n",
        "# - I need a sample size that has interception (if that's possible?)\n",
        "# - I need a sample size with injuries (as dark as that may sound)\n",
        "\n",
        "def clean_2pt_conversion_plays(df_plays, index_start = None):\n",
        "\n",
        "  # Cut 'df_plays' to begin from 'index_start' to the last '2pt conversion' play available in dataframe\n",
        "  if index_start != None:\n",
        "    df_plays_adjusted = df_plays.loc[index_start]\n",
        "    df_2pt_conversion_plays = df_plays_adjusted[df_plays_adjusted['PlayOutcome'].str.contains('Conversion', case=False)]\n",
        "  else:\n",
        "    df_2pt_conversion_plays = df_plays[df_plays['PlayOutcome'].str.contains('Conversion', case=False)]\n",
        "\n",
        "  # Iterating through every 2pt conversion play within 'df_2pt_conversion_plays'\n",
        "  for idx, play in df_2pt_conversion_plays['PlayDescription'].items():\n",
        "\n",
        "    # print(idx)\n",
        "    # print(play)\n",
        "\n",
        "    ############\n",
        "    # REVERSES #\n",
        "    ############\n",
        "\n",
        "    # In 'PlayDescription' all information before the \"reversed\" sentence is not needed.\n",
        "    # - All information before the \"reversed\" sentence is stored within \"ReverseDetails\"\n",
        "    if play.find('REVERSED') != -1:\n",
        "      play_elements = split_play_description(play)\n",
        "      for i in play_elements:\n",
        "        if i.find('REVERSED') != -1:\n",
        "          df_plays.at[idx, 'ReverseDetails'] = play_elements[:play_elements.index(i) + 1]\n",
        "          play = \". \".join(play_elements[play_elements.index(i) + 1:])\n",
        "          break\n",
        "\n",
        "    ###################\n",
        "    # PASSING ATTEMPT #\n",
        "    ###################\n",
        "\n",
        "    pass_2ptc = re.findall(tp_conversion_pass_pattern, play)\n",
        "    if pass_2ptc:\n",
        "      # print(pass_2ptc)\n",
        "      df_plays.loc[idx, 'Passer'] = pass_2ptc[0][0]\n",
        "      df_plays.loc[idx, 'Receiver'] = pass_2ptc[0][1]\n",
        "      df_plays.loc[idx, 'PlayType'] = '2PT Conversion Pass'\n",
        "\n",
        "    ###################\n",
        "    # RUSHING ATTEMPT #\n",
        "    ###################\n",
        "\n",
        "    rush_2ptc = re.findall(tp_conversion_rush_pattern, play)\n",
        "    if rush_2ptc:\n",
        "      # print(rush_2ptc)\n",
        "      df_plays.loc[idx, 'Rusher'] = rush_2ptc[0][0]\n",
        "      # \" \".join(rusher_name[0][1::]).strip()\n",
        "      # df_plays.loc[idx, 'Direction'] = rush_2ptc[0][1]\n",
        "      df_plays.loc[idx, 'Direction'] = \" \".join(rush_2ptc[0][1::]).strip()\n",
        "      df_plays.loc[idx, 'PlayType'] = '2PT Conversion Rush'\n",
        "\n",
        "  return  df_plays"
      ],
      "metadata": {
        "id": "0srpIRjLOhe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DEFENSE CLEANING METHODS"
      ],
      "metadata": {
        "id": "84vArEhwLL8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### INTERCEPTIONS"
      ],
      "metadata": {
        "id": "RWtJnRCNXGah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PURPOSE:\n",
        "# - Clean intercepted plays\n",
        "# INPUT PARAMETERS:\n",
        "# df_plays    - dataframe - dataframe of plays\n",
        "# index_start -  integer  - the starting index of the associated input dataframe\n",
        "#                           to begin cleaning.\n",
        "# RETURN:\n",
        "# df_plays - dataframe - dataframe of plays that now has all useful intercepted play\n",
        "#                        data accessible and clean.\n",
        "\n",
        "# ROUGH DESGIN\n",
        "# 1. Narrow dataframe using 'index_start'\n",
        "#    - This is a recursive method, the narrowing will get smaller and\n",
        "#      smaller until all 'intercepted' type plays have been cleaned.\n",
        "# 2. Grab first 'intercepted' play from narrowed dataframe\n",
        "# 3. Create 2 single row dataframes.\n",
        "#    a. intended play\n",
        "#    b. yardage after interception\n",
        "# 4. Break down play into sentences and clean\n",
        "#    - Depending on the sentence within the play, will determine which\n",
        "#      single row dataframe it will go to.\n",
        "# 5. Combine both dataframes of cleaned data into one dataframe\n",
        "# 6. Replace old play row with new cleaned multi row\n",
        "# 7. return clean_interceped_plays( x , y)\n",
        "#    - x = updated df_plays\n",
        "#    - y = index directly after the last clean added row\n",
        "\n",
        "def clean_intercepted_plays(df_plays, index_start = None):\n",
        "\n",
        "  # Will cut df_plays starting from index_start (narrowing our search space)\n",
        "  if index_start != None:\n",
        "    df_plays_adjusted = df_plays.loc[index_start:]\n",
        "    df_intercepted_plays = df_plays_adjusted[df_plays_adjusted['PlayOutcome'].str.contains('Interception')]\n",
        "  else:\n",
        "    df_intercepted_plays = df_plays[df_plays['PlayOutcome'].str.contains('Interception')]\n",
        "\n",
        "  # Exit case (If no more 'Interception' type plays are found)\n",
        "  if df_intercepted_plays.empty:\n",
        "    return df_plays\n",
        "\n",
        "  # Retrieve the index and 'PlayDescription' of the first intercepted play in 'df_intercepted_plays'\n",
        "  # - Process one play per iteration in the recursive method\n",
        "  idx = df_intercepted_plays.index[0]\n",
        "  play = df_plays['PlayDescription'].loc[idx]\n",
        "\n",
        "  #############\n",
        "  # VARIABLES #\n",
        "  #############\n",
        "\n",
        "  interception_spotting = None\n",
        "\n",
        "  ############\n",
        "  # REVERSES #\n",
        "  ############\n",
        "\n",
        "  # In 'PlayDescription' all information before the \"reversed\" sentence is not needed.\n",
        "  # - All information before the \"reversed\" sentence is stored within \"ReverseDetails\"\n",
        "  if play.find('REVERSED') != -1:\n",
        "    play_elements = split_play_description(play)\n",
        "    for i in play_elements:\n",
        "      if i.find('REVERSED') != -1:\n",
        "        df_plays.at[idx, 'ReverseDetails'] = play_elements[:play_elements.index(i) + 1]\n",
        "        play = \". \".join(play_elements[play_elements.index(i) + 1:])\n",
        "        break\n",
        "\n",
        "  ###########\n",
        "  # FUMBLES #\n",
        "  ###########\n",
        "  # - I am worried about the types of interception fumbles that can happen that I have yet to see.\n",
        "  #   - Such as a fumble by the QB then throws an interception\n",
        "\n",
        "  # Create 2 single row dataframes.\n",
        "  # 1. intended play\n",
        "  df_intended_play = df_plays.loc[idx].copy()\n",
        "  df_intended_play = pd.DataFrame([df_intended_play], columns=df_plays.columns)\n",
        "  df_intended_play.reset_index(drop=True, inplace=True)\n",
        "  df_intended_play['PlayDescription'] = 'nan'\n",
        "  # 2. yardage after interception\n",
        "  df_yardage_after_interception = df_plays.loc[idx].copy()\n",
        "  df_yardage_after_interception = pd.DataFrame([df_yardage_after_interception], columns=df_plays.columns)\n",
        "  df_yardage_after_interception.reset_index(drop=True, inplace=True)\n",
        "  df_yardage_after_interception['PlayDescription'] = 'nan'\n",
        "\n",
        "  # break down play by sentences.\n",
        "  play_elements = split_play_description(play)\n",
        "\n",
        "  # Split play elements\n",
        "  # 1. intended play\n",
        "  #    - Grab all elements leading up to the sentence containing interception\n",
        "  #      - Clean using 'clean_pass_plays' method\n",
        "  # 2. actions after interception\n",
        "  #    - Grab all elements after sentence containing interception\n",
        "  #      - Clean using 'clean_run_plays' method\n",
        "  #      - Clean using 'clean_touchdown_plays' method..?\n",
        "\n",
        "  # Separating play into\n",
        "  # 1. intended passing play\n",
        "  # 2. remaining actions following interception\n",
        "  for i in play_elements:\n",
        "    if i.lower().find('intercepted') != -1:\n",
        "      intended_play_playdescription = \". \".join(play_elements[:play_elements.index(i)+1])\n",
        "      after_interception_playdescription = \". \".join(play_elements[play_elements.index(i)+1:])\n",
        "      # print(idx)\n",
        "      # print(intended_play_playdescription)\n",
        "      # print(after_interception_playdescription)\n",
        "      break\n",
        "\n",
        "  #################\n",
        "  # INTENDED PLAY #\n",
        "  #################\n",
        "\n",
        "  df_intended_play['PlayDescription'] = intended_play_playdescription\n",
        "  df_intended_play['PlayOutcome'] = 'Pass'\n",
        "  df_intended_play = clean_pass_plays(df_intended_play)\n",
        "  df_intended_play['PlayOutcome'] =  df_plays['PlayOutcome'].loc[idx]\n",
        "\n",
        "  # Intercepted by\n",
        "  intercepted_by = re.findall(interception_name_pattern, intended_play_playdescription)\n",
        "  if intercepted_by:\n",
        "    df_intended_play['InterceptedBy'] = intercepted_by[0][0]\n",
        "    interception_spotting = intercepted_by[0][1]\n",
        "    # - During intercepted plays, The intended play portion of the play description is cleaned\n",
        "    #   by the regular pass cleaning method. A defensive player awarded with a pass defend\n",
        "    #   during an intercepted play is formatted the exact same as a player awarded a solo\n",
        "    #   tackle during a completed pass play. I will leverage that here and move the player\n",
        "    #   to the correct feature ('SoloTackle' -> 'PassDefendedBy')\n",
        "    if df_intended_play['SoloTackle'].iloc[0] != 'nan':\n",
        "      df_intended_play.at[0, 'PassDefendedBy'] = (intercepted_by[0][0], df_intended_play['SoloTackle'].iloc[0])\n",
        "      df_intended_play['SoloTackle'] = 'nan'\n",
        "    else:\n",
        "      df_intended_play['PassDefendedBy'] = intercepted_by[0][0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #############################################################\n",
        "  # YARDAGE AFTER INTERCEPTION / TOUCHDOWN AFTER INTERCEPTION #\n",
        "  #############################################################\n",
        "  # - I need this to be able to clean everything.\n",
        "  #   - I need it to be able to clean regular interceptions for yardage (X)\n",
        "  #   - I need it to be able to clean regular interceptions for yardage and then fumbled (X)\n",
        "  #   - I need it to be able to clean interceptions resulting in multiple fumbles (X)\n",
        "  #   - I need it to be able to clean interceptions for touchdowns (X)\n",
        "\n",
        "  #   - I need it to be able to clean a fumbled interception that is recoverd for a touchdown\n",
        "  #   - I need this to account for penalties\n",
        "\n",
        "  # for action in [standard_play_end_pattern]:\n",
        "  for action in [standard_play_end_pattern, touchdown_pattern]:\n",
        "    yardage_after_interception = re.findall(action, after_interception_playdescription)\n",
        "    if yardage_after_interception:\n",
        "      df_yardage_after_interception['PlayDescription'] = after_interception_playdescription\n",
        "\n",
        "      # Flipping team with possession when the play transitions from one team with possession to the other.\n",
        "      if df_yardage_after_interception['TeamWithPossession'].iloc[0] == df_yardage_after_interception['HomeTeam'].iloc[0]:\n",
        "        df_yardage_after_interception['TeamWithPossession'] = df_yardage_after_interception['AwayTeam'].iloc[0]\n",
        "      else:\n",
        "        df_yardage_after_interception['TeamWithPossession'] = df_yardage_after_interception['HomeTeam'].iloc[0]\n",
        "\n",
        "      # - For yardage gained on this play, I would like to send this job to the\n",
        "      #   cleaning method for run plays.\n",
        "      #   - I will need to adjust 3 methods to accomplish this:\n",
        "      #     1. this method\n",
        "      #        - I need to add another regular expession\n",
        "      #          \"defensive_takeaway_run_pattern\"\n",
        "      #     2. run method\n",
        "      #        1. I will need to add another parameter for 'start spotting'\n",
        "      #           - I can grab the start spotting from the end of the sentence\n",
        "      #             containing the intercepted information.\n",
        "      #     3. yardage between spottings\n",
        "      #        - I might have to adjust this method for touchdowns in the\n",
        "      #          future. Right now I think it is capable of doing the trick,\n",
        "      #          but not for touchdowns.\n",
        "\n",
        "      # # Ideally I would like to send this off to another method.\n",
        "      # if action == touchdown_after_takeaway_pattern:\n",
        "      #   df_yardage_after_interception['IsScoringPlay'] = 1\n",
        "      # # else:\n",
        "      df_yardage_after_interception['PlayOutcome'] = 'Run'\n",
        "      df_yardage_after_interception = clean_run_plays(df_yardage_after_interception, interception_spotting)\n",
        "      df_yardage_after_interception['PlayOutcome'] =  df_plays['PlayOutcome'].loc[idx]\n",
        "\n",
        "      # The 'clean_run_plays' method will change 'PlayType' so that is why I am\n",
        "      # putting it down here.\n",
        "      df_yardage_after_interception['PlayType'] = 'Run After Interception'\n",
        "      break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #############################\n",
        "  # NEW REPLACEMENT DATAFRAME #\n",
        "  #############################\n",
        "\n",
        "  # combine both single row dataframes into one\n",
        "  if df_yardage_after_interception['PlayDescription'].iloc[0] == 'nan':\n",
        "    df_cleaned_replacement = df_intended_play\n",
        "  else:\n",
        "    df_cleaned_replacement = pd.concat([df_intended_play, df_yardage_after_interception], ignore_index=True)\n",
        "\n",
        "  # Replace old row with new cleaned dataframe\n",
        "  df_before_row = df_plays.iloc[:df_plays.index.tolist().index(idx)]\n",
        "  df_after_row = df_plays.iloc[df_plays.index.tolist().index(idx)+1:]\n",
        "  df_plays = pd.concat([df_before_row, df_cleaned_replacement, df_after_row], ignore_index=True)\n",
        "\n",
        "  # print()\n",
        "  # If this is the last play in the dataset\n",
        "  if df_intercepted_plays.tail(1).index.tolist()[0] == idx:\n",
        "    return df_plays\n",
        "  else:\n",
        "    return clean_intercepted_plays(df_plays, idx+len(df_cleaned_replacement))"
      ],
      "metadata": {
        "id": "UbRn063DXI6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SACKS"
      ],
      "metadata": {
        "id": "YY6Bv7t8LiaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PURPOSE:\n",
        "# - Clean sacked plays\n",
        "# INPUT PARAMETERS:\n",
        "# df_plays    - dataframe - dataframe of plays\n",
        "# index_start -  integer  - the starting index of the associated input dataframe\n",
        "#                           to begin cleaning.\n",
        "# RETURN:\n",
        "# df_plays - dataframe - dataframe of plays that now has all useful sacked play\n",
        "#                        data accessible and clean.\n",
        "\n",
        "def clean_sacked_plays(df_plays, index_start = None):\n",
        "\n",
        "  # Will cut df_plays starting from index_start (narrowing our search space)\n",
        "  if index_start != None:\n",
        "    df_plays_adjusted = df_plays.iloc[index_start:]\n",
        "    df_sacked_plays = df_plays_adjusted[df_plays_adjusted['PlayOutcome'].str.contains('Sack')]\n",
        "  else:\n",
        "    df_sacked_plays = df_plays[df_plays['PlayOutcome'].str.contains('Sack')]\n",
        "\n",
        "  for idx, play in df_sacked_plays['PlayDescription'].items():\n",
        "    # print(idx)\n",
        "    # print(play)\n",
        "\n",
        "    ############\n",
        "    # REVERSES #\n",
        "    ############\n",
        "\n",
        "    # In 'PlayDescription' all information before the \"reversed\" sentence is not needed.\n",
        "    # - All information before the \"reversed\" sentence is stored within \"ReverseDetails\"\n",
        "    if play.find('REVERSED') != -1:\n",
        "      play_elements = split_play_description(play)\n",
        "      for i in play_elements:\n",
        "        if i.find('REVERSED') != -1:\n",
        "          df_plays.at[idx, 'ReverseDetails'] = play_elements[:play_elements.index(i) + 1]\n",
        "          play = \". \".join(play_elements[play_elements.index(i) + 1:])\n",
        "          break\n",
        "\n",
        "    ###########\n",
        "    # FUMBLES #\n",
        "    ###########\n",
        "    # - Yardage gained from a fumble.. what would this look like?\n",
        "    #   - Would the fumble method completele clean that play?\n",
        "    #     - I think so.\n",
        "    if play.lower().find(\"fumble\") != -1:\n",
        "      helper_clean_fumble_play(df_plays, idx)\n",
        "      if df_sacked_plays.tail(1).index.tolist()[0] == idx:\n",
        "        return df_plays\n",
        "      # continue\n",
        "\n",
        "\n",
        "    ###########\n",
        "    # OFFENSE #\n",
        "    ###########\n",
        "\n",
        "    # Sacked Passer\n",
        "    sacked_passer_name = re.findall(passer_name_pattern, play)\n",
        "    if sacked_passer_name:\n",
        "      df_plays.loc[idx, 'Passer'] = sacked_passer_name[0]\n",
        "\n",
        "    # Sacked Yardage lost\n",
        "    yardage = yardage_between_spottings(df_plays, idx, df_plays.loc[idx, 'FieldPosition'], play)\n",
        "    df_plays.loc[idx, 'Yardage'] = yardage\n",
        "\n",
        "    ###########\n",
        "    # DEFENSE #\n",
        "    ###########\n",
        "\n",
        "    # Solo sack (One person sacked the passer)\n",
        "    solo_sack = re.findall(solo_tackle_pattern, play)\n",
        "    if solo_sack:\n",
        "      df_plays.loc[idx, 'SackedBy'] = solo_sack[0]\n",
        "      df_plays.loc[idx, 'SoloTackle'] = solo_sack[0]\n",
        "\n",
        "    # Split sack (A sack was given to the passer by multiple defenders)\n",
        "    split_sack = re.findall(split_sack_pattern, play)\n",
        "    if split_sack:\n",
        "      df_plays.at[idx, 'SackedBy'] = split_sack[0]\n",
        "      df_plays.at[idx, 'AssistedTackle'] = split_sack[0]\n",
        "\n",
        "    ##############\n",
        "    #  INJURIES  #\n",
        "    ##############\n",
        "\n",
        "    #############\n",
        "    #  PENALTY  #\n",
        "    #############\n",
        "\n",
        "    if df_sacked_plays.tail(1).index.tolist()[0] == idx:\n",
        "      return df_plays"
      ],
      "metadata": {
        "id": "mE4NklmELk6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SPECIAL TEAMS CLEANING METHODS"
      ],
      "metadata": {
        "id": "f8THfFdPuayo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PUNTS"
      ],
      "metadata": {
        "id": "-1ZMhEAyugit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PURPOSE:\n",
        "# - Clean all punt play types\n",
        "\n",
        "# A punt playtype will be split into 2 or more rows\n",
        "#   1. The Punt\n",
        "#      - 'PlayType'\n",
        "#         - Punt\n",
        "#      - 'Punter'\n",
        "#      - 'LongSnapper'\n",
        "#   2. The Punt Return\n",
        "#      - 'PlayType'\n",
        "#         - Punt Return\n",
        "#      - 'PlayOutcome'\n",
        "#         - x yard punt return\n",
        "#         - fair catch\n",
        "#         - touchback\n",
        "#         - out of bounds\n",
        "#         - downed\n",
        "#      - 'Returner'\n",
        "#      - 'Receiver'\n",
        "#      - 'Yardage'\n",
        "#      - 'TackleBy1'\n",
        "#      - 'TackleBy2'\n",
        "#      - 'DownedBy'\n",
        "\n",
        "def clean_punt_plays(df_plays, index_start = None):\n",
        "\n",
        "  # Will cut df_plays starting from index_start (narrowing our search space)\n",
        "  if index_start != None:\n",
        "    df_plays_adjusted = df_plays.loc[index_start:]\n",
        "    df_punt_plays = df_plays_adjusted[df_plays_adjusted['PlayOutcome'].str.contains('Punt')]\n",
        "  else:\n",
        "    df_punt_plays = df_plays[df_plays['PlayOutcome'].str.contains('Punt')]\n",
        "\n",
        "  if df_punt_plays.empty:\n",
        "    return df_plays\n",
        "\n",
        "  # Retrieve the index and 'PlayDescription' of the first punt play in 'df_punt_plays'\n",
        "  # - Process one play per iteration in the recursive method\n",
        "  idx = df_punt_plays.index[0]\n",
        "  play = df_plays['PlayDescription'].loc[idx]\n",
        "\n",
        "  #############\n",
        "  # VARIABLES #\n",
        "  #############\n",
        "\n",
        "  punt_catch_spotting = None\n",
        "\n",
        "  ############\n",
        "  # REVERSES #\n",
        "  ############\n",
        "\n",
        "  # In 'PlayDescription' all information before the \"reversed\" sentence is not needed.\n",
        "  # - All information before is stored within 'ReverseDetails' and the remaining is cleaned.\n",
        "  if play.find('REVERSED') != -1:\n",
        "    play_elements = split_play_description(play)\n",
        "    for i in play_elements:\n",
        "      if i.find(\"REVERSED\") != -1:\n",
        "        # df_play['ReverseDetails'] = play_elements[:play_elements.index(i) + 1]\n",
        "        df_plays.at[idx, 'ReverseDetails'] = play_elements[:play_elements.index(i) + 1]\n",
        "        play = \". \".join(play_elements[play_elements.index(i) + 1:])\n",
        "        break\n",
        "\n",
        "  ###########\n",
        "  # FUMBLES #\n",
        "  ###########\n",
        "  # - I have yet to see a fumble during a punt. I know that it is possible\n",
        "  #   and will have to update this method when that time comes.\n",
        "  # - Fumble returns will be taken care of using 'clean_run_plays'\n",
        "\n",
        "  # Create 2 single row dataframes.\n",
        "  # 1. The Punt\n",
        "  # df_punt = df_play\n",
        "  df_punt = df_plays.loc[idx].copy()\n",
        "  df_punt = pd.DataFrame([df_punt], columns=df_plays.columns)\n",
        "  df_punt.reset_index(drop=True, inplace=True)\n",
        "  df_punt['PlayDescription'] = 'nan'\n",
        "  # 2. The Punt Return\n",
        "  # df_punt_return = df_play\n",
        "  df_punt_return = df_plays.loc[idx].copy()\n",
        "  df_punt_return = pd.DataFrame([df_punt_return], columns=df_plays.columns)\n",
        "  df_punt_return.reset_index(drop=True, inplace=True)\n",
        "  df_punt_return['PlayDescription'] = 'nan'\n",
        "\n",
        "  # break down play by sentences.\n",
        "  play_elements = split_play_description(play)\n",
        "\n",
        "  # Split play elements\n",
        "  # 1. punt\n",
        "  #    - Grab all elements up to the sentence containing punt\n",
        "  # 2. actions after punt\n",
        "  #    - Grab all elements after sentence containing punt\n",
        "  #      - Clean using 'clean_run_plays' method\n",
        "  #      - Clean using 'clean_touchdown_plays' method..?\n",
        "\n",
        "  # Separating play into\n",
        "  # 1. punt play\n",
        "  # 2. remaining actions following punt\n",
        "  for i in play_elements:\n",
        "    if i.lower().find('punts') != -1:\n",
        "      punt_play_playdescription = \". \".join(play_elements[:play_elements.index(i)+1])\n",
        "      punt_return_playdescription = \". \".join(play_elements[play_elements.index(i)+1:])\n",
        "      # print(idx)\n",
        "      # print(punt_play_playdescription)\n",
        "      # print(punt_return_playdescription)\n",
        "      # print()\n",
        "      break\n",
        "\n",
        "  ########\n",
        "  # PUNT #\n",
        "  ########\n",
        "\n",
        "  # All data needed for first row in replacement dataframe\n",
        "  df_punt['PlayDescription'] = punt_play_playdescription\n",
        "  df_punt['PlayOutcome'] = 'Punt'\n",
        "  punt = re.findall(punting_pattern, punt_play_playdescription)\n",
        "  if punt:\n",
        "    punt_catch_spotting = punt[0][2]\n",
        "    df_punt['PlayType'] = 'Punt'\n",
        "    df_punt['PlayDescription'] = i\n",
        "    df_punt['Kicker'] = punt[0][0]\n",
        "    df_punt['Yardage'] = int(punt[0][1])\n",
        "    df_punt['LongSnapper'] = punt[0][3]\n",
        "    # Touchback\n",
        "    if i.find('Touchback') != -1:\n",
        "      df_punt['PlayOutcome'] = 'Touchback'\n",
        "    # Out of bounds\n",
        "    if i.find('out of bounds') != -1:\n",
        "      df_punt['PlayOutcome'] = 'out of bounds'\n",
        "    # Downed by\n",
        "    if i.find('downed by') != -1:\n",
        "      df_punt['PlayOutcome'] = 'downed'\n",
        "      downed_by = re.findall(kick_downed_by_pattern, i)\n",
        "      df_punt['DownedBy'] = downed_by[0]\n",
        "    # fair catch\n",
        "    if i.find('fair catch') != -1:\n",
        "      df_punt['PlayOutcome'] = 'fair catch'\n",
        "      fair_catch_by = re.findall(punt_fair_catch_pattern, i)\n",
        "      df_punt['Returner'] = fair_catch_by[0]\n",
        "\n",
        "  ######################################\n",
        "  # PUNT RETURN (Including touchdowns) #\n",
        "  ######################################\n",
        "\n",
        "  # All data needed for the second row within replacement dataframe\n",
        "  # - Second row only needed when there is a punt return for yardage\n",
        "  # - I think I am going to run into trouble if there is a fumble recovery for yardage\n",
        "  punt_return_patterns = [standard_play_end_pattern]\n",
        "  for return_pattern in punt_return_patterns:\n",
        "    punt_return = re.findall(return_pattern, punt_return_playdescription)\n",
        "    if punt_return:\n",
        "      df_punt_return['PlayDescription'] = punt_return_playdescription\n",
        "      df_punt_return['PlayOutcome'] = 'Run'\n",
        "      # Change team with possession on punt returns to the team that is returning the ball\n",
        "      if df_punt['TeamWithPossession'].iloc[0] == df_punt['HomeTeam'].iloc[0]:\n",
        "        df_punt_return.loc[0, 'TeamWithPossession'] = df_punt['AwayTeam'].iloc[0]\n",
        "      else:\n",
        "        df_punt_return.loc[0, 'TeamWithPossession'] = df_punt['HomeTeam'].iloc[0]\n",
        "      df_punt_return = clean_run_plays(df_punt_return, punt_catch_spotting)\n",
        "      df_punt_return['PlayOutcome'] = df_plays['PlayOutcome'].loc[idx]\n",
        "      df_punt_return['PlayType'] = 'Punt Return'\n",
        "      df_punt_return['Returner'] = df_punt_return['Rusher']\n",
        "      df_punt_return['Rusher'] = 'nan'\n",
        "      break\n",
        "\n",
        "  #############\n",
        "  #  PENALTY  #\n",
        "  #############\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #############################\n",
        "  # NEW REPLACEMENT DATAFRAME #\n",
        "  #############################\n",
        "\n",
        "  if df_punt_return['PlayDescription'].iloc[0] == 'nan':\n",
        "    df_replacement_rows = df_punt\n",
        "  elif df_punt['PlayDescription'].iloc[0] == 'nan': # Will happen during fumbled punt returns.\n",
        "    df_replacement_rows = df_punt_return\n",
        "  else:\n",
        "    df_replacement_rows = pd.concat([df_punt, df_punt_return], ignore_index=True)\n",
        "\n",
        "  df_before_row = df_plays.iloc[:df_plays.index.tolist().index(idx)]\n",
        "  df_after_row = df_plays.iloc[df_plays.index.tolist().index(idx)+1:]\n",
        "  df_plays = pd.concat([df_before_row, df_replacement_rows, df_after_row], ignore_index=True)\n",
        "\n",
        "  if df_punt_plays.tail(1).index.tolist()[0] == idx:\n",
        "    return df_plays\n",
        "  else:\n",
        "    return clean_punt_plays(df_plays, idx+len(df_replacement_rows))"
      ],
      "metadata": {
        "id": "oV3fsB__uhpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### KICKOFF"
      ],
      "metadata": {
        "id": "fdOg2g3qA2DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A kickoff playtype will be split into 1 or more rows\n",
        "\n",
        "# I need to figure out an onside kick (recovered by kicking team)\n",
        "# I need to figure out fumbled kickoff returns\n",
        "# I need to figure out returns for a touchdown\n",
        "# injuries?\n",
        "\n",
        "# Method can mirror punts method.\n",
        "\n",
        "def clean_kickoff_plays(df_plays, index_start = None):\n",
        "\n",
        "  # Will cut df_plays starting from index_start (narrowing our search space)\n",
        "  if index_start != None:\n",
        "    df_plays_adjusted = df_plays.loc[index_start:]\n",
        "    df_kickoff_plays = df_plays_adjusted[df_plays_adjusted['PlayOutcome'].str.contains('Kickoff', case=False)]\n",
        "  else:\n",
        "    df_kickoff_plays = df_plays[df_plays['PlayOutcome'].str.contains('Kickoff', case=False)]\n",
        "\n",
        "  # exit case\n",
        "  if df_kickoff_plays.empty:\n",
        "    return df_plays\n",
        "\n",
        "  # Retrieve the index and 'PlayDescription' of the first kickoff play in 'df_kickoff_plays'\n",
        "  # - Process one play per iteration in the recursive method\n",
        "  idx = df_kickoff_plays.index[0]\n",
        "  play = df_plays['PlayDescription'].loc[idx]\n",
        "\n",
        "  #############\n",
        "  # VARIABLES #\n",
        "  #############\n",
        "\n",
        "  kickoff_catch_spotting = None\n",
        "\n",
        "  ############\n",
        "  # REVERSES #\n",
        "  ############\n",
        "\n",
        "  # In 'PlayDescription' all information before the \"reversed\" sentence is not needed.\n",
        "  # - All information before is stored within 'ReverseDetails' and the remaining is cleaned.\n",
        "  if play.find('REVERSED') != -1:\n",
        "    play_elements = split_play_description(play)\n",
        "    for i in play_elements:\n",
        "      if i.find(\"REVERSED\") != -1:\n",
        "        # df_play['ReverseDetails'] = play_elements[:play_elements.index(i) + 1]\n",
        "        df_plays.at[idx, 'ReverseDetails'] = play_elements[:play_elements.index(i) + 1]\n",
        "        play = \". \".join(play_elements[play_elements.index(i) + 1:])\n",
        "        break\n",
        "\n",
        "  ###########\n",
        "  # FUMBLES #\n",
        "  ###########\n",
        "  # - Will be taken care of using 'clean_run_plays'\n",
        "\n",
        "  # Create 2 single row dataframes.\n",
        "  # 1. The Kickoff\n",
        "  df_kickoff = df_plays.loc[idx].copy()\n",
        "  df_kickoff = pd.DataFrame([df_kickoff], columns=df_plays.columns)\n",
        "  df_kickoff.reset_index(drop=True, inplace=True)\n",
        "  df_kickoff['PlayDescription'] = 'nan'\n",
        "  # 2. The Kickoff Return\n",
        "  df_kickoff_return = df_plays.loc[idx].copy()\n",
        "  df_kickoff_return = pd.DataFrame([df_kickoff_return], columns=df_plays.columns)\n",
        "  df_kickoff_return.reset_index(drop=True, inplace=True)\n",
        "  df_kickoff_return['PlayDescription'] = 'nan'\n",
        "\n",
        "  # break down play by sentences.\n",
        "  play_elements = split_play_description(play)\n",
        "\n",
        "  # Split play elements\n",
        "  # 1. kickoff\n",
        "  #    - Grab all elements up to the sentence containing kickoff\n",
        "  # 2. actions after kickoff\n",
        "  #    - Grab all elements after sentence containing kickoff\n",
        "  #      - Clean using 'clean_run_plays' method\n",
        "  #      - Clean using 'clean_touchdown_plays' method..?\n",
        "\n",
        "  # Separating play into\n",
        "  # 1. punt play\n",
        "  # 2. remaining actions following punt\n",
        "  for i in play_elements:\n",
        "    if i.lower().find('kicks') != -1:\n",
        "      kickoff_play_playdescription = \". \".join(play_elements[:play_elements.index(i)+1])\n",
        "      kickoff_return_playdescription = \". \".join(play_elements[play_elements.index(i)+1:])\n",
        "      if kickoff_return_playdescription.find(\"(didn't try to advance)\") != -1:\n",
        "        kickoff_return_playdescription = kickoff_return_playdescription.replace(\"(didn't try to advance) \", \"\")\n",
        "      # print(idx)\n",
        "      # print(kickoff_play_playdescription)\n",
        "      # if len(kickoff_return_playdescription) > 0:\n",
        "      #   print(idx)\n",
        "      #   print(kickoff_play_playdescription)\n",
        "      #   print(kickoff_return_playdescription)\n",
        "      #   print()\n",
        "      break\n",
        "\n",
        "  ###########\n",
        "  # KICKOFF #\n",
        "  ###########\n",
        "\n",
        "  # All data needed for first row in replacement dataframe\n",
        "  df_kickoff['PlayDescription'] = kickoff_play_playdescription\n",
        "  df_kickoff['PlayOutcome'] = 'Kickoff'\n",
        "  kickoff = re.findall(kickoff_pattern, kickoff_play_playdescription)\n",
        "  if kickoff:\n",
        "    # print(kickoff)\n",
        "    kickoff_catch_spotting = kickoff[0][3]\n",
        "    # Change team with possession on kickoff to the team that is kicking\n",
        "    if df_kickoff['TeamWithPossession'].iloc[0] == df_kickoff['HomeTeam'].iloc[0]:\n",
        "      df_kickoff_return.loc[0, 'TeamWithPossession'] = df_kickoff['AwayTeam'].iloc[0]\n",
        "    else:\n",
        "      df_kickoff_return.loc[0, 'TeamWithPossession'] = df_kickoff['HomeTeam'].iloc[0]\n",
        "    df_kickoff['Kicker'] = kickoff[0][0]\n",
        "    # df_kickoff['Yardage'] = int(kickoff[0][1]) <--- use helper method\n",
        "    if kickoff_play_playdescription.find('Touchback') != -1:\n",
        "      df_kickoff['PlayOutcome'] = 'Touchback'\n",
        "    # I need to figure out what the difference will be when the kicking team recovers\n",
        "    if kickoff_play_playdescription.find('onside') != -1:\n",
        "      df_kickoff['PlayOutcome'] = 'onside'\n",
        "      downed_by = re.findall(kick_downed_by_pattern, i)\n",
        "      if downed_by:\n",
        "        df_kickoff['DownedBy'] = downed_by[0]\n",
        "\n",
        "  #########################################\n",
        "  # KICKOFF RETURN (Including touchdowns) #\n",
        "  #########################################\n",
        "\n",
        "\n",
        "\n",
        "  # 1. might have to create new regular expression for kickoff return patterns\n",
        "  # 2. might have to adjust yardage between spottings method to accomidate for\n",
        "  #    kickoff yardage\n",
        "  #    - Does yardage between spotting handle 'end zone' as an input..?\n",
        "\n",
        "\n",
        "\n",
        "  # All data needed for the second row within replacement dataframe\n",
        "  # - Second row only needed when there is a kickoff return for yardage\n",
        "  kickoff_return_patterns = [standard_play_end_pattern]\n",
        "  for return_pattern in kickoff_return_patterns:\n",
        "    kickoff_return = re.findall(return_pattern, kickoff_return_playdescription)\n",
        "    if kickoff_return:\n",
        "      # print(kickoff_return)\n",
        "      df_kickoff_return['PlayDescription'] = kickoff_return_playdescription\n",
        "      df_kickoff_return['PlayOutcome'] = 'Run'\n",
        "      # Change team with possession on kickoff returns to the team that is\n",
        "      # returning the ball.\n",
        "      if df_kickoff['TeamWithPossession'].iloc[0] == df_kickoff['HomeTeam'].iloc[0]:\n",
        "        df_kickoff_return.loc[0, 'TeamWithPossession'] = df_kickoff['AwayTeam'].iloc[0]\n",
        "      else:\n",
        "        df_kickoff_return.loc[0, 'TeamWithPossession'] = df_kickoff['HomeTeam'].iloc[0]\n",
        "      df_kickoff_return = clean_run_plays(df_kickoff_return, kickoff_catch_spotting)\n",
        "      df_kickoff_return['PlayOutcome'] = df_plays['PlayOutcome'].loc[idx]\n",
        "      df_kickoff_return['PlayType'] = 'Punt Return'\n",
        "      df_kickoff_return['Returner'] = df_kickoff_return['Rusher']\n",
        "      df_kickoff_return['Rusher'] = 'nan'\n",
        "      break\n",
        "\n",
        "  #############\n",
        "  #  PENALTY  #\n",
        "  #############\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #############################\n",
        "  # NEW REPLACEMENT DATAFRAME #\n",
        "  #############################\n",
        "\n",
        "  if df_kickoff_return['PlayDescription'].iloc[0] == 'nan':\n",
        "    df_replacement_rows = df_kickoff\n",
        "  elif df_kickoff['PlayDescription'].iloc[0] == 'nan': # Will happen during fumbled punt returns.\n",
        "    df_replacement_rows = df_kickoff_return\n",
        "  else:\n",
        "    df_replacement_rows = pd.concat([df_kickoff, df_kickoff_return], ignore_index=True)\n",
        "\n",
        "  df_before_row = df_plays.iloc[:df_plays.index.tolist().index(idx)]\n",
        "  df_after_row = df_plays.iloc[df_plays.index.tolist().index(idx)+1:]\n",
        "  df_plays = pd.concat([df_before_row, df_replacement_rows, df_after_row], ignore_index=True)\n",
        "\n",
        "  if df_kickoff_plays.tail(1).index.tolist()[0] == idx:\n",
        "    return df_plays\n",
        "  else:\n",
        "    return clean_kickoff_plays(df_plays, idx+len(df_replacement_rows))"
      ],
      "metadata": {
        "id": "T_zGfiroA4Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SCORING CLEANING METHODS"
      ],
      "metadata": {
        "id": "ULJzn953ORhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TOUCHDOWNS"
      ],
      "metadata": {
        "id": "CffLVTfsazgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_touchdown_plays(df_plays, index_start=None):\n",
        "\n",
        "  # Cut 'df_plays' to begin from 'index_start' to the last touchdown play available in dataframe\n",
        "  if index_start != None:\n",
        "    df_plays_adjusted = df_plays.loc[index_start:]\n",
        "    df_touchdown_plays = df_plays_adjusted[df_plays_adjusted['PlayOutcome'].str.contains('Touchdown')]\n",
        "  else:\n",
        "    df_touchdown_plays = df_plays[df_plays['PlayOutcome'].str.contains('Touchdown')]\n",
        "\n",
        "  # Iterating through every touchdown play within 'df_touchdown_plays'\n",
        "  for idx, play in df_touchdown_plays['PlayDescription'].items():\n",
        "\n",
        "\n",
        "    ##########################\n",
        "    # INTERCEPTED TOUCHDOWNS #\n",
        "    ##########################\n",
        "\n",
        "    # Still need to clean intercepted play types\n",
        "    if play.find(\"INTERCEPTED\") != -1:\n",
        "\n",
        "      print(idx)\n",
        "      print(play)\n",
        "\n",
        "      # creating a copy of the incercepted touchdown play and cleaning the copy\n",
        "      intercepted_touchdown_row = df_plays.loc[idx].copy()\n",
        "      intercepted_touchdown_row['PlayOutcome'] = 'Interception'\n",
        "      intercepted_touchdown_row['IsScoringPlay'] = 1 # This will only be the value for the team that threw the interception\n",
        "      intercepted_touchdown_row = pd.DataFrame([intercepted_touchdown_row], columns=df_plays.columns)\n",
        "      intercepted_touchdown_row.reset_index(drop=True, inplace=True)\n",
        "\n",
        "      #################################################################################################### Under Construction\n",
        "      # Change feature 'TeamWithPossession' for each play in drive\n",
        "      # - Raw data states that the team that intercepted the ball for a touchdown had possession for each play\n",
        "      #   within drive. The correct value for this feature for each play in drive is the team that threw\n",
        "      #   the interception.\n",
        "\n",
        "      wrong_team_with_possession = df_plays['TeamWithPossession'].loc[idx]\n",
        "      if wrong_team_with_possession == df_plays['HomeTeam'].loc[idx]:\n",
        "        correct_team_with_possession = df_plays['AwayTeam'].loc[idx]\n",
        "      else:\n",
        "        correct_team_with_possession = df_plays['HomeTeam'].loc[idx]\n",
        "\n",
        "      # HERE I NEED TO CHANGE ALL 'TEAMWITHPOSSESSION' FEATURES FOR EVERY PLAY IN DRIVE\n",
        "      # I need to figure out how to efficiently grab every play in drive.\n",
        "      intercepted_touchdown_row['TeamWithPossession'] = correct_team_with_possession\n",
        "      conditions_for_unique_drive = ((df_plays['Season'] == df_plays['Season'].loc[idx]) &\n",
        "      (df_plays['Week'] == df_plays['Week'].loc[idx]) &\n",
        "      (df_plays['AwayTeam'] == df_plays['AwayTeam'].loc[idx]) &\n",
        "      (df_plays['HomeTeam'] == df_plays['HomeTeam'].loc[idx]) &\n",
        "      (df_plays['Quarter'] == df_plays['Quarter'].loc[idx]) &\n",
        "      (df_plays['DriveNumber'] == df_plays['DriveNumber'].loc[idx]))\n",
        "\n",
        "      df_plays.loc[conditions_for_unique_drive, 'TeamWithPossession'] = correct_team_with_possession\n",
        "\n",
        "      ####################################################################################################\n",
        "\n",
        "      # Because this is an interception for a touchdown, the defensive team should have their team\n",
        "      # with possession to end the drive.\n",
        "      # REMINDER: This single play is separated into multiple actions (play will be represented with multiple rows)\n",
        "      intercepted_touchdown_row = clean_intercepted_plays(intercepted_touchdown_row)\n",
        "      intercepted_touchdown_row['PlayOutcome'] = df_plays['PlayOutcome'].loc[idx]\n",
        "\n",
        "      # Replacing old row with cleaned row\n",
        "      df_before_row = df_plays.iloc[:df_plays.index.tolist().index(idx)]\n",
        "      df_after_row = df_plays.iloc[df_plays.index.tolist().index(idx)+1:]\n",
        "      df_plays = pd.concat([df_before_row, intercepted_touchdown_row, df_after_row], ignore_index=True)\n",
        "\n",
        "      print()\n",
        "\n",
        "      # Recursion to update 'df_plays'\n",
        "      if df_touchdown_plays.tail(1).index.tolist()[0] == idx:\n",
        "        return df_plays\n",
        "      else:\n",
        "        return clean_touchdown_plays(df_plays, idx+len(intercepted_touchdown_row))\n",
        "\n",
        "    ######################\n",
        "    # PASSING TOUCHDOWNS #\n",
        "    ######################\n",
        "\n",
        "    # If a play has a passer throwing the ball, I am assuming it is a passing play\n",
        "    passing_play = re.findall(passer_name_pattern, play)\n",
        "    if len(passing_play) > 0 and play.find(\"sacked\") == -1 and play.find(\"INTERCEPTED\") == -1:\n",
        "      # print(idx)\n",
        "      # print(play)\n",
        "\n",
        "      # creating a copy of the passing touchdown play row and cleaning the copy\n",
        "      passing_touchdown_row = df_plays.loc[idx].copy()\n",
        "      passing_touchdown_row['PlayType'] = 'Pass'\n",
        "      passing_touchdown_row['PlayOutcome'] = 'Pass'\n",
        "      passing_touchdown_row['IsScoringPlay'] = 1\n",
        "      passing_touchdown_row = pd.DataFrame([passing_touchdown_row], columns=df_plays.columns)\n",
        "      passing_touchdown_row = clean_pass_plays(passing_touchdown_row)\n",
        "      passing_touchdown_row['PlayOutcome'] = df_plays['PlayOutcome'].loc[idx]\n",
        "\n",
        "      # Replacing old row with cleaned row\n",
        "      df_before_row = df_plays.iloc[:df_plays.index.tolist().index(idx)]\n",
        "      df_after_row = df_plays.iloc[df_plays.index.tolist().index(idx)+1:]\n",
        "      df_plays = pd.concat([df_before_row, passing_touchdown_row, df_after_row], ignore_index=True)\n",
        "\n",
        "      # print()\n",
        "\n",
        "      # Recursion to update 'df_plays'\n",
        "      if df_touchdown_plays.tail(1).index.tolist()[0] == idx:\n",
        "        return df_plays\n",
        "      else:\n",
        "        return clean_touchdown_plays(df_plays, idx+len(passing_touchdown_row))\n",
        "\n",
        "  return df_plays"
      ],
      "metadata": {
        "id": "6JtOJENWOU9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FIELD GOALS"
      ],
      "metadata": {
        "id": "wa4M-wQNRy5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I need an example of when a player returns the field goal for yardage\n",
        "# I need a larger sample size for \"Blocked\" field goals\n",
        "# I need to figure out what to do if someone fumbles a recovery\n",
        "# I need to figure out what to do on a trick play (e.i. holder runs out with the ball)\n",
        "# - INCOMPLETE. NEED LARGER SAMPLE SIZE\n",
        "\n",
        "def clean_field_goal_plays(df_plays, index_start = None):\n",
        "\n",
        "  # Adjusting df_plays to start cleaning at a specified index (index_start)\n",
        "  if index_start != None:\n",
        "    df_plays_adjusted = df_plays.loc[index_start:]\n",
        "    # Locating all field goal plays within dataframe\n",
        "    df_field_goal_plays = df_plays_adjusted[df_plays_adjusted['PlayOutcome'].str.contains('Field Goal')]\n",
        "  else:\n",
        "    # Locating all field goal plays within dataframe\n",
        "    df_field_goal_plays = df_plays[df_plays['PlayOutcome'].str.contains('Field Goal')]\n",
        "\n",
        "  for idx, play in df_field_goal_plays['PlayDescription'].items():\n",
        "\n",
        "    # print(idx)\n",
        "    # print(play)\n",
        "\n",
        "    ###################\n",
        "    # EXTRA PLAY DATA #\n",
        "    ###################\n",
        "\n",
        "    #########################\n",
        "    # FIELD GOAL SITUATIONS #\n",
        "    #########################\n",
        "\n",
        "    field_goal = re.findall(field_goal_pattern, play)\n",
        "    if field_goal:\n",
        "      # print(field_goal)\n",
        "      df_plays.loc[idx, 'PlayType'] = 'Field Goal'\n",
        "      df_plays.loc[idx, 'Kicker'] = field_goal[0][0]\n",
        "      df_plays.loc[idx, 'Yardage'] = int(field_goal[0][1])\n",
        "      # Element will only fill if the field goal was no good\n",
        "      if field_goal[0][2] != '':\n",
        "        df_plays.loc[idx, 'Direction'] = field_goal[0][2]\n",
        "      df_plays.loc[idx, 'LongSnapper'] = field_goal[0][3]\n",
        "      df_plays.loc[idx, 'Holder'] = field_goal[0][4]\n",
        "\n",
        "    ######################\n",
        "    # FIELD GOAL BLOCKED #\n",
        "    ######################\n",
        "\n",
        "    field_goal_blocked = re.findall(field_goal_blocked_pattern, play)\n",
        "    if field_goal_blocked:\n",
        "      print(idx)\n",
        "      print(play)\n",
        "      print(field_goal_blocked)\n",
        "\n",
        "      # Because of the potential recovery for yardage after a blocked field\n",
        "      # goal, I need 2 dataframes:\n",
        "      # 1. intended field goal attempt (containing blocked details)\n",
        "      df_blocked_fg = df_plays.loc[idx].copy()\n",
        "      df_blocked_fg = pd.DataFrame([df_blocked_fg], columns=df_plays.columns)\n",
        "      df_blocked_fg.reset_index(drop=True, inplace=True)\n",
        "      df_blocked_fg['PlayDescription'] = 'nan'\n",
        "      # 2. yardage after recovery\n",
        "      df_yardage_after_recovery = df_plays.loc[idx].copy()\n",
        "      df_yardage_after_recovery = pd.DataFrame([df_yardage_after_recovery], columns=df_plays.columns)\n",
        "      df_yardage_after_recovery.reset_index(drop=True, inplace=True)\n",
        "      df_yardage_after_recovery['PlayDescription'] = 'nan'\n",
        "\n",
        "      # I need to separate the play description into 2 groups:\n",
        "      # 1. All actions involving the field goal and field goal block\n",
        "      #    - for df_blocked_fg\n",
        "      # 2. All actions following the field goal block recovery for yardage\n",
        "      #    - for df_yardage_after_recovery\n",
        "      play_elements = split_play_description(play)\n",
        "      for i in play_elements:\n",
        "        if i.lower().find('blocked') != -1:\n",
        "          blocked_fg_playdescription = \". \".join(play_elements[:play_elements.index(i)+1])\n",
        "          yardage_after_recovery_playdescription = \". \".join(play_elements[play_elements.index(i)+1:])\n",
        "          print(blocked_fg_playdescription)\n",
        "          print(yardage_after_recovery_playdescription)\n",
        "\n",
        "      #####################\n",
        "      # BOCKED FIELD GOAL #\n",
        "      #####################\n",
        "\n",
        "      df_blocked_fg['PlayDescription'] = blocked_fg_playdescription\n",
        "      df_blocked_fg['PlayType'] = 'Field Goal'\n",
        "      blocked_fg_data = re.findall(field_goal_blocked_pattern, blocked_fg_playdescription)\n",
        "      df_blocked_fg['Kicker'] = blocked_fg_data[0][0]\n",
        "      df_blocked_fg['Yardage'] = int(blocked_fg_data[0][1])\n",
        "      df_blocked_fg['BlockedBy'] = blocked_fg_data[0][2]\n",
        "      df_blocked_fg['LongSnapper'] = blocked_fg_data[0][3]\n",
        "      df_blocked_fg['Holder'] = blocked_fg_data[0][4]\n",
        "\n",
        "      ###################################\n",
        "      # FIELD GOAL RECOVERY FOR YARDAGE #\n",
        "      ###################################\n",
        "\n",
        "      # If there was a recovery for yardage\n",
        "      if re.findall(standard_play_end_pattern, yardage_after_recovery_playdescription):\n",
        "        df_yardage_after_recovery['PlayDescription'] = yardage_after_recovery_playdescription\n",
        "        df_yardage_after_recovery['PlayOutcome'] = 'Run'\n",
        "        # blocked_fg_data[0][5] - team of the player that recovered the ball\n",
        "        df_yardage_after_recovery['TeamWithPossession'] = blocked_fg_data[0][5]\n",
        "        # blocked_fg_data[0][7] - recovery spotting of player who recovered ball\n",
        "        df_yardage_after_recovery = clean_run_plays(df_yardage_after_recovery, blocked_fg_data[0][7])\n",
        "        df_yardage_after_recovery['PlayOutcome'] = df_plays['PlayOutcome'].loc[idx]\n",
        "        df_yardage_after_recovery['PlayType'] = 'Field Goal Return'\n",
        "\n",
        "      #################\n",
        "      # NEW DATAFRAME #\n",
        "      #################\n",
        "      # - If there was a recovery for yardage, there will be multiple rows\n",
        "      #   within the dataframe of plays that represent the blocked field goal\n",
        "      #   along with actions that followed.\n",
        "\n",
        "      if df_yardage_after_recovery['PlayDescription'].iloc[0] == 'nan':\n",
        "        df_replacement_rows = df_blocked_fg\n",
        "      else:\n",
        "        df_replacement_rows = pd.concat([df_blocked_fg, df_yardage_after_recovery], ignore_index=True)\n",
        "\n",
        "      df_before_row = df_plays.iloc[:df_plays.index.tolist().index(idx)]\n",
        "      df_after_row = df_plays.iloc[df_plays.index.tolist().index(idx)+1:]\n",
        "      df_plays = pd.concat([df_before_row, df_replacement_rows, df_after_row], ignore_index=True)\n",
        "\n",
        "      if df_field_goal_plays.tail(1).index.tolist()[0] == idx:\n",
        "        return df_plays\n",
        "      else:\n",
        "        return clean_field_goal_plays(df_plays, idx+len(df_replacement_rows))\n",
        "\n",
        "      print()\n",
        "\n",
        "  return df_plays"
      ],
      "metadata": {
        "id": "AAy5023BR32t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EXTRA POINTS"
      ],
      "metadata": {
        "id": "umFY2-YmILcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_extra_point_plays(df_plays, index_start = None):\n",
        "\n",
        "  # Adjusting df_plays to start cleaning at a specified index (index_start)\n",
        "  if index_start != None:\n",
        "    df_plays_adjusted = df_plays.loc[index_start:]\n",
        "    # Locating all field goal plays within dataframe\n",
        "    df_field_goal_plays = df_plays_adjusted[df_plays_adjusted['PlayOutcome'].str.contains('Extra Point')]\n",
        "  else:\n",
        "    # Locating all field goal plays within dataframe\n",
        "    df_field_goal_plays = df_plays[df_plays['PlayOutcome'].str.contains('Extra Point')]\n",
        "\n",
        "  for idx, play in df_field_goal_plays['PlayDescription'].items():\n",
        "\n",
        "    print(idx)\n",
        "    print(play)\n",
        "\n",
        "    ###################\n",
        "    # EXTRA PLAY DATA #\n",
        "    ###################\n",
        "\n",
        "\n",
        "    ##########################\n",
        "    # EXTRA POINT SITUATIONS #\n",
        "    ##########################\n",
        "\n",
        "    extra_point = re.findall(extra_point_pattern, play)\n",
        "    if extra_point:\n",
        "      print(extra_point)\n",
        "      df_plays.loc[idx, 'PlayType'] = 'Extra Point'\n",
        "      df_plays.loc[idx, 'Kicker'] = extra_point[0][0]\n",
        "      if extra_point[0][1] != '':\n",
        "        df_plays.loc[idx, 'Direction'] = extra_point[0][1]\n",
        "      df_plays.loc[idx, 'LongSnapper'] = extra_point[0][2]\n",
        "      df_plays.loc[idx, 'Holder'] = extra_point[0][2]\n",
        "\n",
        "    print()\n",
        "\n",
        "  return df_plays"
      ],
      "metadata": {
        "id": "1vwmOyZJIiuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. PIPELINE MAIN METHOD"
      ],
      "metadata": {
        "id": "jAT4815Z2EQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PURPOSE:\n",
        "# - Accept a dataframe of nfl plays (formatted by NFL_Scrapers) and\n",
        "#   return a cleaned dataframe of those plays.\n",
        "# INPUT PARAMETERS:\n",
        "# df_all_plays         - dataframe - all plays in raw form from NFL_Scraper that user\n",
        "#                                    would like to clean.\n",
        "# OUTPUT:\n",
        "# df_all_plays_cleaned - dataframe - all plays from 'df_all_plays' cleaned and data\n",
        "#                                    dispersed into individual new features.\n",
        "\n",
        "# CURRENT DESIGN PLAN:\n",
        "# 1. Use uniquely designed methods for each play type to clean within dataframe\n",
        "#    - (e.g. pass, run, touchdown, punt, sack, ... )\n",
        "# 2. Repeat until all plays within dataframe have been cleaned.\n",
        "#   NOTE:\n",
        "#   - It is important to fully clean a play type before moving to the next\n",
        "#      because sometimes cleaning could involve adding a new row to the dataframe,\n",
        "#      causing a reset to the dataframes indexing.\n",
        "#      - If we were to separate all play types from the beginning, the indexes\n",
        "#        could shift around causing, for example, an index that might originally\n",
        "#        point to a run play to now instead point at a pass play.\n",
        "\n",
        "def clean_dataframe_of_plays(df_all_plays):\n",
        "\n",
        "  # Return Dataframe\n",
        "  df_all_plays_cleaned = df_all_plays.copy()\n",
        "\n",
        "  ################################\n",
        "  # RAW DATA COLUMN DESCRIPTIONS #\n",
        "  ################################\n",
        "\n",
        "  # Season             - Year of the season\n",
        "  # Week               - Game week of the season (e.g. 'Week 1')\n",
        "  # Day                - Day of the week (e.g. 'MON')\n",
        "  # Date               - Month and day of the game formatted MM/DD (e.g. '09/07')\n",
        "  # AwayTeam           - Visiting team of the game\n",
        "  # HomeTeam           - Home team of the game\n",
        "  # Quarter            - Quarter that the play is in\n",
        "  #                      - NOT ACCURATE. Drives that go between quarters will end up\n",
        "  #                        having all plays in the later quarter.\n",
        "  # DriveNumber        - Drive number of the quarter that the play is in\n",
        "  # TeamWithPossession - Team that started with the ball at the beginning of the play.\n",
        "  # IsScoringDrive     - Does the drive that the focused play in result in a score?\n",
        "  # PlayNumberInDrive  - Play count in the drive\n",
        "  # IsScoringPlay      - Did the play result in a score?\n",
        "  # PlayOutcome        - Ultimate result of the play (e.g. '13 Yard Pass')\n",
        "  # PlayStart          - The down and where the play started on the field (e.g. '2nd & 9 at DET 21')\n",
        "  # PlayTimeFormation  - Time left in the quarter / quarter / play formation\n",
        "  # PlayDescription    - The raw description given of the focused play, entailing everything\n",
        "  #                      that happened within it.\n",
        "\n",
        "  #############################################################\n",
        "  # TRANSFORMING FEATURE VALUES (PREPPING DATA TO BE CLEANED) #\n",
        "  #############################################################\n",
        "  df_all_plays_cleaned = playtimeformation_split(df_all_plays_cleaned)\n",
        "  df_all_plays_cleaned = playstart_split(df_all_plays_cleaned)\n",
        "  df_all_plays_cleaned = consistent_team_names(df_all_plays_cleaned)\n",
        "\n",
        "  ######################################\n",
        "  # NEW ADDITIONAL COLUMN DESCRIPTIONS #\n",
        "  ######################################\n",
        "\n",
        "  # ~ General features ~\n",
        "  # TimeOnTheClock     - NOT HERE ANYMORE.\n",
        "\n",
        "  # ~ Offensive features ~\n",
        "  # EndSpot            - Where the end of the play has been spotted\n",
        "  #                      - This can also be where the end of the action within a play has been spotted.\n",
        "  # PlayType           - The type of play (e.g. pass/run)\n",
        "  # Formation          - Play formation\n",
        "  # Passer             - Player that threw the ball (mostly the quarterback)\n",
        "  # Rusher             - Player that ran the ball (mostly the runningback)\n",
        "  # Receiver           - Player on the same team as the passer that caught the ball\n",
        "  # Direction          - Where the ball is going during the play\n",
        "  # Yardage            - Yards gained during the play\n",
        "  #                      - (Should specify that yardage does not include extra yardage gained from penalties)\n",
        "  #                      - (Player awarded yardage)\n",
        "  #                      - (also includes how far kicks have gone during kickoffs and punts)\n",
        "\n",
        "  # ~ Defensive features ~\n",
        "  # SoloTackle         - Player awarded a solo tackle from a play\n",
        "  # AssistedTackle     - Player awarded an assisted tackle from a play\n",
        "  # SharedTackle       - Player awarded a shared tackle from a play\n",
        "  # PassDefendedBy     - Defender that defended the passing play\n",
        "  # PressureBy         - Defender that applied pressure to the passer\n",
        "  # InterceptedBy      - Defender that intercepted the passing play\n",
        "  # SackedBy           - Player awarded a sack from a play. (Could be solo or split)\n",
        "  # ForcedFumbledBy    - Player awarded a forced fumble from a play\n",
        "\n",
        "  # ~ Unique features (uncommon) ~\n",
        "  # WhoFumbled         - Player who last held the ball during a fumble.\n",
        "  # FumbleRecoveredBy  - Player who recovered the fumbled ball\n",
        "  # FumbleDetails      - A list that has what happened after the fumble\n",
        "  #                      - [forced fumble by, recovered by, yards gained, tackled by]\n",
        "  # ReverseDetails     - A list having plays leading up to play reversal\n",
        "  # InjuredPlayers     - Players that were injured during the play\n",
        "  # AcceptedPenalty    - Penalty on the field that was accepted\n",
        "  # DeclinedPenalty    - Penalty on the field that was declined\n",
        "\n",
        "  # ~ Special teams features ~\n",
        "  # Kicker             - Player who kicked the ball during a kickoff / punt / extra point / field goal\n",
        "  # LongSnapper        - Player who snapped the ball during a punt / extra point / field goal\n",
        "  # Returner           - Player who returned the ball during a kickoff / punt\n",
        "  # DownedBy           - ? ? ? I forget\n",
        "  # Holder             - Player who held ball for extra point / field goal\n",
        "  # BlockedBy          - Player who blocked a punt / extra point / field goal\n",
        "\n",
        "  new_columns = [\"EndSpot\",\n",
        "                 \"PlayType\", \"Passer\", \"Rusher\", \"Receiver\", \"Direction\", \"Yardage\",\n",
        "                 \"SoloTackle\", \"AssistedTackle\", \"SharedTackle\", 'PassDefendedBy', \"PressureBy\", \"InterceptedBy\", \"SackedBy\", \"ForcedFumbleBy\",\n",
        "                 \"WhoFumbled\", \"FumbleRecoveredBy\", \"FumbleDetails\", \"ReverseDetails\", \"InjuredPlayers\", \"AcceptedPenalty\", \"DeclinedPenalty\",\n",
        "                 \"Kicker\", \"LongSnapper\", \"Returner\", \"DownedBy\", \"Holder\", \"BlockedBy\"]\n",
        "\n",
        "  string_columns = [\"EndSpot\",\n",
        "                    \"PlayType\", \"Passer\", \"Rusher\", \"Receiver\", \"Direction\",\n",
        "                    \"SoloTackle\", \"AssistedTackle\", \"SharedTackle\", 'PassDefendedBy', \"PressureBy\", \"InterceptedBy\", \"SackedBy\", \"ForcedFumbleBy\",\n",
        "                    \"WhoFumbled\", \"FumbleRecoveredBy\", \"FumbleDetails\", \"ReverseDetails\", \"InjuredPlayers\", \"AcceptedPenalty\", \"DeclinedPenalty\",\n",
        "                    \"Kicker\", \"LongSnapper\", \"Returner\", \"DownedBy\", \"Holder\", \"BlockedBy\"]\n",
        "\n",
        "  int_columns = [\"Yardage\"]\n",
        "\n",
        "  ########################################\n",
        "  # RETURN DATAFRAME WITH ADDED FEATURES #\n",
        "  ########################################\n",
        "\n",
        "  df_all_plays_cleaned = df_all_plays_cleaned.reindex(columns=df_all_plays_cleaned.columns.tolist() + new_columns)\n",
        "  df_all_plays_cleaned[string_columns] = df_all_plays_cleaned[string_columns].astype(str)\n",
        "  df_all_plays_cleaned[int_columns] = df_all_plays_cleaned[int_columns].astype(float)\n",
        "\n",
        "  ########################################\n",
        "  # GETTING PLAY CATEGORIES AND CLEANING #\n",
        "  ########################################\n",
        "  # TOUCHDOWNS MUST BE CLEANED FIRST\n",
        "  # - Any touchdown resulting from a change in possession (e.g. Interception for Touchdown)\n",
        "  #   raw data states that the team on defense had possession the entire drive.\n",
        "  #   - So all plays leading up to the touchdown state that the defense has possession.\n",
        "  df_all_plays_cleaned = clean_touchdown_plays(df_all_plays_cleaned)\n",
        "  # df_all_plays_cleaned = clean_pass_plays(df_all_plays_cleaned)\n",
        "  # df_all_plays_cleaned = clean_run_plays(df_all_plays_cleaned)\n",
        "  # df_all_plays_cleaned = clean_2pt_conversion_plays(df_all_plays_cleaned)\n",
        "  # df_all_plays_cleaned = clean_intercepted_plays(df_all_plays_cleaned)\n",
        "  # df_all_plays_cleaned = clean_sacked_plays(df_all_plays_cleaned)\n",
        "  # df_all_plays_cleaned = clean_punt_plays(df_all_plays_cleaned)\n",
        "  # df_all_plays_cleaned = clean_kickoff_plays(df_all_plays_cleaned)\n",
        "  # df_all_plays_cleaned = clean_field_goal_plays(df_all_plays_cleaned)\n",
        "  # df_all_plays_cleaned = clean_extra_point_plays(df_all_plays_cleaned)\n",
        "\n",
        "\n",
        "\n",
        "  return df_all_plays_cleaned"
      ],
      "metadata": {
        "id": "HnBvmNQk2Nb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTING"
      ],
      "metadata": {
        "id": "OWowcpZjtb9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_week2_plays_cleaned = clean_dataframe_of_plays(week2_2023_plays)"
      ],
      "metadata": {
        "id": "xwGlU2XquSc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_week2_plays_cleaned.shape"
      ],
      "metadata": {
        "id": "mI4RUuSuuZYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_week2_plays_cleaned"
      ],
      "metadata": {
        "id": "18vENGlYStB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PLAYTYPE OBSERVATIONS"
      ],
      "metadata": {
        "id": "IYc-or2funxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modifying plays to match cleaned plays transformed features\n",
        "# ( e.g. Quarter(original) = '1st Quarter\n",
        "#        Quarter(transform) = 1 )\n",
        "# - This is needed in order to match plays from the original dataframe\n",
        "#   to the cleaned dataframe.\n",
        "df_week2_plays_modified = week2_2023_plays.copy()\n",
        "df_week2_plays_modified = playtimeformation_split(df_week2_plays_modified)\n",
        "df_week2_plays_modified = playstart_split(df_week2_plays_modified)\n",
        "df_week2_plays_modified = consistent_team_names(df_week2_plays_modified)"
      ],
      "metadata": {
        "id": "tV9nnkc25Kfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HELPER METHOD"
      ],
      "metadata": {
        "id": "44hbpB7VviFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PURPOSE:\n",
        "# - A tool that can be used to compare original plays and their cleaned versions\n",
        "\n",
        "# I would like to return a map that has:\n",
        "# KEY: index of original unclean play\n",
        "# VALUE: index(es) of cleaned play\n",
        "\n",
        "def unclean_to_clean_play_matches(df_unclean_plays, df_clean_plays):\n",
        "\n",
        "  my_map = {}\n",
        "\n",
        "  # This list of features is unique to each play\n",
        "  # - Both the unclean and cleaned versions of the plays have these same features, therefore\n",
        "  #   they will be used to match unclean plays in 'df_unclean_plays' to clean plays in 'df_clean_plays'\n",
        "  matching_features = ['Season', 'Week', 'Date', 'AwayTeam', 'HomeTeam', 'Quarter', 'DriveNumber', 'PlayNumberInDrive']\n",
        "\n",
        "  # Iterate through each row of the unclean plays dataframe\n",
        "  for u_row in df_unclean_plays.itertuples(index=True):\n",
        "    u_features = [getattr(u_row, col) for col in matching_features]\n",
        "\n",
        "    matching_indexes = []\n",
        "    matches_found = False\n",
        "\n",
        "    # Iterate through each row of the dataframe of cleaned plays\n",
        "    # - The starting index will be the index of the unclean play within the main original dataframe of plays\n",
        "    #   - The matching cleaned pair will either be at the exact same location or higher\n",
        "    for c_row in df_clean_plays[u_row.Index::].itertuples(index=True):\n",
        "      c_features = [getattr(c_row, col) for col in matching_features]\n",
        "\n",
        "      # If a match is found, check for consective rows of matches because some uncleaned plays needed to be cleaned using multiple rows\n",
        "      # - Once a row that does not match follows one that does, will break the loop because the one play match has been found.\n",
        "      if u_features == c_features:\n",
        "        matching_indexes.append(c_row.Index)\n",
        "        matches_found = True\n",
        "      elif matches_found:\n",
        "        my_map[u_row.Index] = matching_indexes\n",
        "        break\n",
        "\n",
        "  return my_map"
      ],
      "metadata": {
        "id": "vm2CpfHJvg3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OFFENSIVE PLAYS"
      ],
      "metadata": {
        "id": "eX9zBvEMJFgT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PASSING PLAYS"
      ],
      "metadata": {
        "id": "ZNlQga6VuzkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All passing plays\n",
        "df_unclean_pass_plays = df_week2_plays_modified.loc[(df_week2_plays_modified['PlayOutcome'].str.contains('Pass'))]\n",
        "\n",
        "map_unclean_clean_pass_plays = unclean_to_clean_play_matches(df_unclean_pass_plays, df_week2_plays_cleaned)\n",
        "\n",
        "len(map_unclean_clean_pass_plays.keys())\n",
        "\n",
        "# # All passing plays to 'A.St. Brown'\n",
        "# # - I need to figure out how to separate each sentence of the play description. Currently I am splitting them\n",
        "# #   by finding this set of characters \". \", This will not work all the time and might actually cause error because\n",
        "# #   some players have names that have \". \" in them and this will cause the splitting to be at their name.\n",
        "# df_unclean_pass_plays = df_week2_plays_modified.loc[(df_week2_plays_modified['PlayOutcome'].str.contains('Pass')) &\n",
        "#                                                     (df_week2_plays_modified['PlayDescription'].str.contains('A.St. Brown', case=False))]\n",
        "\n",
        "# map_unclean_clean_pass_plays = unclean_to_clean_play_matches(df_unclean_pass_plays, df_week2_plays_cleaned)\n",
        "\n",
        "# len(map_unclean_clean_pass_plays.keys())"
      ],
      "metadata": {
        "id": "nTMz4Wq-uthP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Every unclean passing play and their associated cleaned play breakdown\n",
        "\n",
        "for i in map_unclean_clean_pass_plays.keys():\n",
        "  print(f\"({i}, {map_unclean_clean_pass_plays.get(i)})\")\n",
        "  play = df_week2_plays_modified['PlayDescription'].iloc[i]\n",
        "  # play_split = play.split(\". \")\n",
        "  play_split = split_play_description(play)\n",
        "  for j in play_split:\n",
        "    print(j)\n",
        "  print()"
      ],
      "metadata": {
        "id": "gNPmMY_d8_n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RUN PLAYS"
      ],
      "metadata": {
        "id": "rFRPLX5oPFE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All rushing plays\n",
        "df_unclean_run_plays = df_week2_plays_modified.loc[(df_week2_plays_modified['PlayOutcome'].str.contains('Run'))]\n",
        "\n",
        "map_unclean_clean_run_plays = unclean_to_clean_play_matches(df_unclean_run_plays, df_week2_plays_cleaned)\n",
        "\n",
        "len(map_unclean_clean_run_plays.keys())"
      ],
      "metadata": {
        "id": "R3Hls8qjTqLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Every unclean run play and their associated cleaned play breakdown\n",
        "\n",
        "for i in map_unclean_clean_run_plays.keys():\n",
        "  print(f\"({i}, {map_unclean_clean_run_plays.get(i)})\")\n",
        "  play = df_week2_plays_modified['PlayDescription'].iloc[i]\n",
        "  play_split = play.split(\". \")\n",
        "  for j in play_split:\n",
        "    print(j)\n",
        "  print()"
      ],
      "metadata": {
        "id": "fkPyGKNjUL7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2PT CONVERSION"
      ],
      "metadata": {
        "id": "NFZdKrzb52i-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_2023_2pt_conversion_week2 = week2_2023_plays[week2_2023_plays['PlayOutcome'].str.contains('Conversion')]\n",
        "\n",
        "# All 2PT conversion attempts\n",
        "df_unclean_conversion_plays = df_week2_plays_modified.loc[(df_week2_plays_modified['PlayOutcome'].str.contains('Conversion'))]\n",
        "\n",
        "map_unclean_clean_conversion_plays = unclean_to_clean_play_matches(df_unclean_conversion_plays, df_week2_plays_cleaned)"
      ],
      "metadata": {
        "id": "7xEyhMB856g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Every unclean conversion play and their associated cleaned play breakdown\n",
        "\n",
        "for i in map_unclean_clean_conversion_plays.keys():\n",
        "  print(f\"({i}, {map_unclean_clean_conversion_plays.get(i)})\")\n",
        "  play = df_week2_plays_modified['PlayDescription'].iloc[i]\n",
        "  play_split = play.split(\". \")\n",
        "  for j in play_split:\n",
        "    print(j)\n",
        "  print()"
      ],
      "metadata": {
        "id": "VTGCyzyd6RBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DEFENSIVE PLAYS"
      ],
      "metadata": {
        "id": "vwennCDeJLD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### INTERCEPTION"
      ],
      "metadata": {
        "id": "TdvXP3PDgVgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_2023_interception_week2 = week2_2023_plays[week2_2023_plays['PlayOutcome'].str.contains('Interception')]\n",
        "\n",
        "# All interception attempts\n",
        "df_unclean_interception_plays = df_week2_plays_modified.loc[(df_week2_plays_modified['PlayOutcome'].str.contains('Interception'))]\n",
        "\n",
        "map_unclean_clean_interception_plays = unclean_to_clean_play_matches(df_unclean_interception_plays, df_week2_plays_cleaned)"
      ],
      "metadata": {
        "id": "GJWdB40zgX3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Every unclean interception play and their associated cleaned play breakdown\n",
        "\n",
        "for i in map_unclean_clean_interception_plays.keys():\n",
        "  print(f\"({i}, {map_unclean_clean_interception_plays.get(i)})\")\n",
        "  play = df_week2_plays_modified['PlayDescription'].iloc[i]\n",
        "  play_split = play.split(\". \")\n",
        "  for j in play_split:\n",
        "    print(j)\n",
        "  print()"
      ],
      "metadata": {
        "id": "h8gQUHqWgr1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SACK"
      ],
      "metadata": {
        "id": "PWzncjhWJS-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_2023_sack_week2 = week2_2023_plays[week2_2023_plays['PlayOutcome'].str.contains('Sack')]\n",
        "\n",
        "# All sacks\n",
        "df_unclean_sack_plays = df_week2_plays_modified.loc[(df_week2_plays_modified['PlayOutcome'].str.contains('Sack'))]\n",
        "\n",
        "map_unclean_clean_sack_plays = unclean_to_clean_play_matches(df_unclean_sack_plays, df_week2_plays_cleaned)"
      ],
      "metadata": {
        "id": "XrPG0d_rJWCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Every unclean sacked play and their associated cleaned play breakdown\n",
        "\n",
        "for i in map_unclean_clean_sack_plays.keys():\n",
        "  print(f\"({i}, {map_unclean_clean_sack_plays.get(i)})\")\n",
        "  play = df_week2_plays_modified['PlayDescription'].iloc[i]\n",
        "  play_split = play.split(\". \")\n",
        "  for j in play_split:\n",
        "    print(j)\n",
        "  print()"
      ],
      "metadata": {
        "id": "PzXkoQeEJnjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SPECIAL TEAMS PLAYS"
      ],
      "metadata": {
        "id": "Is4VUno1umTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PUNTS"
      ],
      "metadata": {
        "id": "VbIhpLjpwFkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_2023_punt_week2 = week2_2023_plays[week2_2023_plays['PlayOutcome'].str.contains('Punt')]\n",
        "\n",
        "# All punts\n",
        "df_unclean_punt_plays = df_week2_plays_modified.loc[(df_week2_plays_modified['PlayOutcome'].str.contains('Punt')) &\n",
        "                                                    (df_week2_plays_modified['PlayDescription'].str.contains('FUMBLE'))]\n",
        "\n",
        "map_unclean_clean_punt_plays = unclean_to_clean_play_matches(df_unclean_punt_plays, df_week2_plays_cleaned)"
      ],
      "metadata": {
        "id": "AVRFuMNTvfol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Every unclean punt play and their associated cleaned play breakdown\n",
        "\n",
        "for i in map_unclean_clean_punt_plays.keys():\n",
        "  print(f\"({i}, {map_unclean_clean_punt_plays.get(i)})\")\n",
        "  play = df_week2_plays_modified['PlayDescription'].iloc[i]\n",
        "  play_split = split_play_description(play)\n",
        "  for j in play_split:\n",
        "    print(j)\n",
        "  print()"
      ],
      "metadata": {
        "id": "Dx1w8ct8wSNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KICKOFFS"
      ],
      "metadata": {
        "id": "piCO0D8ADw2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_2023_kickoff_week2 = week2_2023_plays[week2_2023_plays['PlayOutcome'].str.contains('Kickoff')]\n",
        "\n",
        "# All punts\n",
        "df_unclean_kickoff_plays = df_week2_plays_modified.loc[(df_week2_plays_modified['PlayOutcome'].str.contains('Kickoff')) &\n",
        "                                                       (df_week2_plays_modified['PlayDescription'].str.contains('FUMBLES'))]\n",
        "\n",
        "map_unclean_clean_kickoff_plays = unclean_to_clean_play_matches(df_unclean_kickoff_plays, df_week2_plays_cleaned)"
      ],
      "metadata": {
        "id": "_sofsCpYDyt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Every unclean kickoff play and their associated cleaned play breakdown\n",
        "\n",
        "for i in map_unclean_clean_kickoff_plays.keys():\n",
        "  print(f\"({i}, {map_unclean_clean_kickoff_plays.get(i)})\")\n",
        "  play = df_week2_plays_modified['PlayDescription'].iloc[i]\n",
        "  play_split = split_play_description(play)\n",
        "  for j in play_split:\n",
        "    print(j)\n",
        "  print()"
      ],
      "metadata": {
        "id": "xKOl-x1wD8V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SCORING PLAYS"
      ],
      "metadata": {
        "id": "tg6RvlZXSmdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TOUCHDOWNS"
      ],
      "metadata": {
        "id": "Sqi-HAKtn36G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_2023_touchdown_week2 = week2_2023_plays[week2_2023_plays['PlayOutcome'].str.contains('Touchdown')]\n",
        "\n",
        "# All touchdowns\n",
        "df_unclean_touchdown_plays = df_week2_plays_modified.loc[(df_week2_plays_modified['PlayOutcome'].str.contains('Touchdown'))]\n",
        "\n",
        "map_unclean_clean_touchdown_plays = unclean_to_clean_play_matches(df_unclean_touchdown_plays, df_week2_plays_cleaned)"
      ],
      "metadata": {
        "id": "AAkvSn-an7f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Every unclean touchdown play and their associated cleaned play breakdown\n",
        "\n",
        "for i in map_unclean_clean_touchdown_plays.keys():\n",
        "  print(f\"({i}, {map_unclean_clean_touchdown_plays.get(i)})\")\n",
        "  play = df_week2_plays_modified['PlayDescription'].iloc[i]\n",
        "  play_split = split_play_description(play)\n",
        "  for j in play_split:\n",
        "    print(j)\n",
        "  print()"
      ],
      "metadata": {
        "id": "UGrNo4L5opP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FIELD GOALS"
      ],
      "metadata": {
        "id": "fbf9MCGcSuZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_2023_field_goal_week2 = week2_2023_plays[week2_2023_plays['PlayOutcome'].str.contains('Field Goal')]\n",
        "\n",
        "# All field goals\n",
        "df_unclean_field_goal_plays = df_week2_plays_modified.loc[(df_week2_plays_modified['PlayOutcome'].str.contains('Field Goal')) &\n",
        "                                                          (df_week2_plays_modified['PlayDescription'].str.contains('BLOCKED'))]\n",
        "\n",
        "map_unclean_clean_field_goal_plays = unclean_to_clean_play_matches(df_unclean_field_goal_plays, df_week2_plays_cleaned)"
      ],
      "metadata": {
        "id": "nw6OG-iRSxkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Every unclean field goal play and their associated cleaned play breakdown\n",
        "\n",
        "for i in map_unclean_clean_field_goal_plays.keys():\n",
        "  print(f\"({i}, {map_unclean_clean_field_goal_plays.get(i)})\")\n",
        "  play = df_week2_plays_modified['PlayDescription'].iloc[i]\n",
        "  play_split = split_play_description(play)\n",
        "  for j in play_split:\n",
        "    print(j)\n",
        "  print()"
      ],
      "metadata": {
        "id": "ia5tTsGLTEOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXTRA POINTS"
      ],
      "metadata": {
        "id": "02Km3sZUG9wI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_2023_extra_point_week2 = week2_2023_plays[week2_2023_plays['PlayOutcome'].str.contains('Extra Point')]\n",
        "\n",
        "# All field goals\n",
        "df_unclean_extra_point_plays = df_week2_plays_modified.loc[(df_week2_plays_modified['PlayOutcome'].str.contains('Extra Point'))]\n",
        "\n",
        "map_unclean_clean_extra_point_plays = unclean_to_clean_play_matches(df_unclean_extra_point_plays, df_week2_plays_cleaned)"
      ],
      "metadata": {
        "id": "VRgXI4kPG_t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Every unclean field goal play and their associated cleaned play breakdown\n",
        "\n",
        "for i in map_unclean_clean_extra_point_plays.keys():\n",
        "  print(f\"({i}, {map_unclean_clean_extra_point_plays.get(i)})\")\n",
        "  play = df_week2_plays_modified['PlayDescription'].iloc[i]\n",
        "  play_split = split_play_description(play)\n",
        "  for j in play_split:\n",
        "    print(j)\n",
        "  print()"
      ],
      "metadata": {
        "id": "jyfTx6WXHaQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INDEX SEARCHING"
      ],
      "metadata": {
        "id": "iSyccUYrX-yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_week2_plays_cleaned.iloc[803]"
      ],
      "metadata": {
        "id": "2ZHn8U_nYDbF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}